<!DOCTYPE html>
<html lang="en">

<head>
	<meta charset="UTF-8">
	<meta name="viewport" id="view" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
	<meta name="google-site-verification" content="GxaFv2GEAd_nIt2RJk2O1uJ6vcNiCshwywxJC2v2kQE" />
	<meta name="msvalidate.01" content="67B155C505E04C672096FC5B5891EF73" />
	<meta name="description"
		content="Here is Huanxuan Liao's personal homepage, including CV, papers, interests and so on.">
	<meta name="keywords" content="huanxuan liao, xnhyacinth, Huanxuan Liao, huanxuan, 廖桓萱">
	<link rel="stylesheet" type="text/css" media="screen and (max-device-width:400px)" href="tinyScreen.css" />
	<link rel="stylesheet" type="text/css" media="screen and (min-width: 400px)and (max-device-width: 600px)"
		href="smallScreen.css" />
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
	<!-- <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap"> -->
	<!-- <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'"> -->
	<!-- <link rel="stylesheet" href="/css/vendor-bundle.min.047268c6dd09ad74ba54a0ba71837064.css" media="print" onload="this.media='all'"> -->
	<!-- <link rel="stylesheet" href="/css/wowchemy.042e26407c9e383d96a1f26d6787c686.css" /> -->
	<!-- <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" > -->
	<!-- <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled> -->
	<!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.2/css/academicons.min.css" integrity="sha512-KlJCpRsLf+KKu2VQa5vmRuClRFjxc5lXO03ixZt82HZUk41+1I0bD8KBSA0fY290ayMfWYI9udIqeOWSu1/uZg==" crossorigin="anonymous" media="print" onload="this.media='all'"> -->
	<link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css">
	<!-- <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/6.5.1/css/font-awesome.css"> -->
	<!-- <link rel="apple-touch-icon" sizes="57x57" href="icon/apple-icon-57x57.png">
	<link rel="apple-touch-icon" sizes="60x60" href="icon/apple-icon-60x60.png">
	<link rel="apple-touch-icon" sizes="72x72" href="icon/apple-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="76x76" href="icon/apple-icon-76x76.png">
	<link rel="apple-touch-icon" sizes="114x114" href="icon/apple-icon-114x114.png">
	<link rel="apple-touch-icon" sizes="120x120" href="icon/apple-icon-120x120.png">
	<link rel="apple-touch-icon" sizes="144x144" href="icon/apple-icon-144x144.png">
	<link rel="apple-touch-icon" sizes="152x152" href="icon/apple-icon-152x152.png">
	<link rel="apple-touch-icon" sizes="180x180" href="icon/apple-icon-180x180.png">
	<link rel="icon" type="image/png" sizes="192x192" href="icon/android-icon-192x192.png">
	<link rel="icon" type="image/png" sizes="32x32" href="icon/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="96x96" href="icon/favicon-96x96.png">
	<link rel="icon" type="image/png" sizes="16x16" href="icon/favicon-16x16.png"> -->
	<link rel="icon" type="image/png" sizes="32x32" href="assets/icon/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="180x180" href="assets/icon/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="16x16" href="assets/icon/favicon-16x16.png">
	<link rel="icon" type="image/png" sizes="192x192" href="assets/icon/android-chrome-192x192.png">
	<link rel="icon" type="image/png" sizes="512x512" href="assets/icon/android-chrome-512x512.png">
	<link rel="manifest" href="icon/manifest.json">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css">
	<meta name="msapplication-TileColor" content="#ffffff">
	<meta name="msapplication-TileImage" content="assets/icon/favicon-32x32.pngg">
	<meta name="theme-color" content="#ffffff">
	<meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests" />
	<title>Huanxuan Liao | Homepage</title>
	<style type="text/css">
		h1 {
			text-align: center;
			font-size: 1.5em;
		}
	</style>
	<script async defer src="https://buttons.github.io/buttons.js"></script>
</head>

<body>
	<!--     <h1>Seeking for a Master Degree...</h1>   -->
	<br>
</body>

</html>

<!DOCTYPE html>
<html>

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<style media="screen" type="text/css">
		/* 		<link rel="shortcut icon" type="image/x-icon" href="favicon.ico" /> */
		body {
			border: 0pt none;
			font-family: inherit;
			font-size: 100%;
			font-style: inherit;
			font-weight: inherit;
			margin: 0pt;
			outline-color: invert;
			outline-style: none;
			outline-width: 0pt;
			padding: 0pt;
			vertical-align: baseline;
		}

		/* body {
			position: relative;
			margin: 3em auto 2em auto;
			width: 68em;
			font-family: Times New Roman, Lato, Verdana, Helvetica, sans-serif;
			font-size: 1.0em;
			background: #fdfdfd;
		} */
		/* 替换原有的 body 样式 */
		body {
			position: relative;
			max-width: 70%;

			margin: 3em auto 2em;
			padding: 0 10px;
			font-family: Times New Roman, Lato, Verdana, Helvetica, sans-serif;
			font-size: 1.0em;
			background: #fdfdfd;
		}


		@media (max-width: 1200px) {
			body {
				max-width: 92%;
			}
		}

		@media (max-width: 768px) {
			body {
				max-width: 95%;
				margin: 2em auto;
			}
		}

		@media (max-width: 480px) {
			body {
				max-width: 100%;
				padding: 0 10px;
				margin: 1em 0;
			}
		}
	</style>
	<!-- <style>
			body {
			  color: #333;
			  background-color: #eee;
			}
			@media (prefers-color-scheme: dark) {
			  body {
				color: #ccc;
				background-color: #1f1f1f;
			  }
			}
		</style> -->
	<style>
		/* 普通模式下颜色 */
		:root {
			--color-bg: #f2f2f2;
			--color-xg: #cbcbcb;
			--color-ch: #30303a;
			--color-btn: #f0f0fa;
			--color-link: #3d8ee4;
			/*1573df*/
			--color-border: #0b46dc;
			--color-b: #292727;
			--color-div: #d0d0d0;
			--color-my: #0000ff;
			--color-p: #1198c1;
			--color-low: #65656e;
		}

		/* 黑暗模式下颜色 */
		:root[theme='dark'] {
			--color-bg: #272935;
			--color-xg: #111217;
			--color-ch: #e3e3f1;
			--color-btn: #30303a;
			--color-link: #a2bfd9;
			--color-border: #f2d3d3;
			--color-b: #e9adad;
			--color-div: #4b4b4b;
			--color-my: #2d67db;
			--color-p: #86abcb;
			--color-low: #d0cccc;
		}

		:root * {
			/* 过渡动画效果 */
			transition: background-color 0.3s, color 0.3s;
		}

		body {
			position: relative;
			/* background: linear-gradient(to bottom,
					var(--color-bg) 0%,
					var(--color-bg) 96.5%,
					var(--color-xg) 96.5%,
					var(--color-xg) 100%); */
			background-color: var(--color-bg);
			/* transition: 0.3s; */
		}

		button {
			position: absolute;
			top: -1%;
			right: -4em;
			height: 3em;
			width: 3em;
			border-radius: 50%;
			border: none;
			outline: none;
			background-color: var(--color-btn);
			display: flex;
			align-items: center;
			justify-content: center;
			/* transition: 0.3s; */
		}

		a:link {
			text-decoration: underline;
			TEXT-DECORATION: none;
		}

		a:visited {
			text-align: left;
			text-decoration: underline;
			TEXT-DECORATION: none;
		}

		a:hover {
			color: #ec8435;
			text-decoration: underline;
			TEXT-DECORATION: none;
		}

		a:active {
			color: #de123b;
			text-decoration: none;
		}

		p,
		b,
		td,
		h2,
		i,
		a,
		ul,
		ul li,
		pre,
		textarea {
			color: var(--color-ch);
			font-family: Georgia, Times, serif;
			/* transition: 0.3s; */
		}

		.myself {
			color: var(--color-my)
		}

		.paper {
			color: var(--color-p)
		}

		pre {
			font-size: 0.76em;
		}

		ul li {
			font-size: 1.3em;
		}

		.dy {
			color: var(--color-link);
			font-family: Georgia, Times, serif;
			/* transition: 0.3s; */
		}

		i,
		a {
			color: var(--color-link);
			font-family: Georgia, Times, serif;
			/* transition: 0.3s; */
		}

		.news {
			color: var(--color-b);
		}

		textarea {
			background-color: var(--color-bg);
			width: 63em;
			/* width: 100%; */
			/* box-sizing: border-box; */
			font-size: 0.76em;
			resize: none;
			border: 1.8px solid var(--color-border);
			padding-left: 0.6em;
			padding-top: 0.5em;
			min-height: 100px;
		}

		textarea_div {
			background-color: var(--color-bg);
			resize: none;
			width: 63em;
			font-size: 0.76em;
			border: 1.8px solid var(--color-border);
			padding-left: 0.6em;
			padding-top: 0.5em;
			min-height: 100px;
			overflow-x: hidden;
			overflow-y: auto;

		}

		[contentEditable=true]:empty:not(:focus):before {
			content: attr(data-text);
		}


		i:hover {
			color: #eb7f7f
		}

		.aaa:hover {
			color: #ec90c7
		}

		button>svg {
			height: 3em;
			width: 3em;
			fill: var(--color-ch);
			stroke: none;
		}

		button>svg:nth-child(2) {
			display: none;
		}

		nav {
			background-color: var(--color-bg);
			padding: 10px;
			font-family: Georgia, Times, serif;
		}

		nav a {
			text-decoration: none;
			color: var(--color-link);
			padding: 10px 20px;
			position: relative;
			transition: all 0.3s ease;
		}

		nav a:hover {
			text-decoration: none;
		}

		nav a::before {
			content: "";
			position: absolute;
			bottom: 0;
			left: 0;
			width: 0;
			height: 3px;
			background-color: gray;
			transition: width 0.3s ease;
		}

		nav a:hover::before {
			width: 100%;
		}

		.divider {
			border-top: 1px solid var(--color-div);
			margin: 10px 0 10px 0;
			width: 100%;
		}

		.divide {
			border-top: 1px solid var(--color-div);
			margin: -5px 0 -5px 0;
		}

		@media (prefers-color-scheme: dark) {

			/* 普通模式下颜色 */
			:root {
				--color-bg: #272935;
				--color-xg: #111217;
				--color-ch: #e3e3f1;
				--color-btn: #30303a;
				--color-link: #a2bfd9;
				--color-border: #f2d3d3;
				--color-b: #e9adad;
				--color-div: #4b4b4b;
				--color-my: #2d67db;
				--color-p: #86abcb;
				--color-low: #d0cccc;
				;
			}

			/* 黑暗模式下颜色 */
			:root[theme='dark'] {
				--color-bg: #f2f2f2;
				--color-xg: #cbcbcb;
				--color-ch: #30303a;
				--color-btn: #f0f0fa;
				--color-link: #3d8ee4;
				/*1573df*/
				--color-border: #0b46dc;
				--color-b: #292727;
				--color-div: #d0d0d0;
				--color-my: #0000ff;
				--color-p: #1198c1;
				--color-low: #65656e;
			}
		}
	</style>

	<script type="text/javascript">
		function autoResize(id) {
			const textArea = document.getElementById(id);
			textArea.style.height = "auto";
			textArea.style.height = (textArea.scrollHeight) + "px";
		}
	</script>

	<script>
		var _hmt = _hmt || [];
		(function () {
			var hm = document.createElement("script");
			hm.src = "https://hm.baidu.com/hm.js?1ec4ad5c61857459aa78d5ee7ddee28d";
			var s = document.getElementsByTagName("script")[0];
			s.parentNode.insertBefore(hm, s);
		})();
	</script>
	
	<script src="https://cdn.staticfile.org/jquery/2.0.0/jquery.min.js">
	</script>

	<script>
		$(function () {
			$(".bu").click(function () {
				if ($('.bib' + $(this).attr("id")).css("display") != "none") {
					$('.bib' + $(this).attr("id")).css("display", "none");
				} else {
					if ($('.ab' + $(this).attr("id")).css("display") != "none") {
						$('.ab' + $(this).attr("id")).css("display", "none");
					}
					$('.bib' + $(this).attr("id")).css("display", "block");
				}
				const textArea = document.getElementById('mybib' + $(this).attr("id"));
				textArea.style.height = "auto";
				textArea.style.height = (textArea.scrollHeight) + "px";
			})
			$(".au").click(function () {
				if ($('.ab' + $(this).attr("id")).css("display") != "none") {
					$('.ab' + $(this).attr("id")).css("display", "none");
				} else {
					if ($('.bib' + $(this).attr("id")).css("display") != "none") {
						$('.bib' + $(this).attr("id")).css("display", "none");
					}
					$('.ab' + $(this).attr("id")).css("display", "block");
				}
				const textArea = document.getElementById('myab' + $(this).attr("id"));
				textArea.style.height = "auto";
				textArea.style.height = (textArea.scrollHeight) + "px";
			})
		})

		$(function () {
			$.fn.autoHeight = function () {
				function autoHeight(elem) {
					elem.style.height = 'auto';
					elem.scrollTop = 0; //防抖动
					elem.style.height = elem.scrollHeight + 'px';
				}
				this.each(function () {
					autoHeight(this);
					$(this).on('keyup', function () {
						autoHeight(this);
					});
				});
			}
			$('textarea[autoHeight]').autoHeight();
		})
	</script>
</head>

<body>
	<button>
		<svg viewBox="0 0 24 24">
			<path
				d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z">
			</path>
		</svg>
		<svg viewBox="0 0 24 24">
			<path
				d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z">
			</path>
		</svg>
	</button>
	<script>
		const root = document.documentElement;
		const button = document.querySelector('button');
		const svgs = document.querySelectorAll('button>svg');
		const text = document.querySelector('p');

		button.onclick = () => {
			if (!root.hasAttribute('theme')) { // 检查当前主题
				root.setAttribute('theme', 'dark'); // 向根节点插入theme属性，值为dark
				// 这样页面中颜色就会匹配 :root[theme='dark'] { ... } 这一套
				svgs[0].style.display = 'none';
				svgs[1].style.display = 'block';

			} else {
				root.removeAttribute('theme'); // 移除根节点theme属性
				svgs[1].style.display = 'none';
				svgs[0].style.display = 'block';

			}
		};
	</script>
	<script type="text/javascript">
		//获取设备类型

		var ua = navigator.userAgent;

		var ipad = ua.match(/(iPad).*OS\s([\d_]+)/),

			isIphone = !ipad && ua.match(/(iPhone\sOS)\s([\d_]+)/),

			isAndroid = ua.match(/(Android)\s+([\d.]+)/),

			isMobile = isIphone || isAndroid;

		//判断

		if (isMobile) {
			//获取设备像素
			var devicewidth = document.documentElement.clientWidth;
			var devicewidth2 = window.screen.width;
			console.log(devicewidth2);
			console.log(devicewidth);
			//按照比例 设置id为view的content值为scale，ok
			var scale = devicewidth / 1200; //1200是我原本设计PC页面固定的大小，根据你自己页面进行修改
			console.log(scale);

			document.getElementById("view").setAttribute('content', "user-scalable=yes, width=device-width, initial-scale=" + scale);

		} else {

		}


	</script>
	<nav>
		<!-- <b>
			<font size="5"><a href="https://xnhyacinth.github.io" target="_self">Huanxuan Liao (<font face="宋体">廖桓萱</font>)</a></font>
		</b> -->
		<b>
			<font size="5"><a href="https://xnhyacinth.github.io" target="_self">Biography</a></font>
		</b>
		<b>
			<font size="5"><a href="#publications" target="_self">Publications</a></font>
		</b>
		<!-- <a href="#">Talks</a> -->
		<b>
			<font size="5"><a href="#awards" target="_self">Awards</a></font>
		</b>
		<b>
			<font size="5"><a href="#services" target="_self">Services</a></font>
		</b>
		<!-- <a href="#">Misc</a> -->
		<b>
			<font size="5"><a href="https://xnhyacinth.github.io/CV.pdf" target="_self">CV (PDF)</a></font>
		</b>
	</nav>
	<!-- <hr /> -->
	<div class="divider"></div>
	<table align="center">
		<!-- <tr> -->
		<td align="center"><a href="assets/img/profile.jpg" target="_self"><img border=0 style="border-radius:50%;"
					height="228em" src="assets/img/profile_compress.jpg"></a>
		</td>
		<!--height="218em" width="198em"-->
		<td align="center">&nbsp</td>
		<td align="center">&nbsp</td>
		<td align="center">
		<td align="center">
			<h2>
				<font face="Roboto" size=+3>Huanxuan Liao</font>&emsp;
				<b>
					<font face="宋体" size=+3>廖桓萱</font>
				</b>
			</h2>
			<p>
				<font size=+1>
					<font size=+2 face="Georgia">👨‍💻 PhD Student @ <a href="https://www.ucas.ac.cn/"
							target="_self">UCAS</a> focusing on LLM 🎓</font><br />
					<font size=+1>&#x1F30F</font> <b><a href="http://www.ia.cas.cn/" target="_self">
							<font face="Georgia">Institute of
								Automation, Chinese Academy of Sciences</font>
						</a></b>
					<font face="Georgia">, Beijing, China</font>
				</font>
			</p>
			<p style="margin-top: -0.4cm; margin-bottom: 0.2cm;">
				<font size=+1>
					<font size=+2>&#x2709</font> <i><b><a href="mailto:liaohuanxuan2023@ia.ac.cn">
								<font size=+1 face="Georgia">liaohuanxuan2023@ia.ac.cn</font></b></i></a>
				</font>
			</p>

			<div style="display: inline-block; margin-top:0%">
				<a href="assets/CV.pdf" target="_self">
					<!-- <font size=7><i class="fa fa-user-circle" aria-hidden="true"></i></font> -->
					<font size=4><i class="ai ai-cv ai-3x"></i></font>
				</a>
			</div>
			&emsp;
			<!-- Google Scholar -->
			<div style="display: inline-block; margin-top:0%">
				<a href="https://scholar.google.com.hk/citations?user=sRcWOKUAAAAJ&hl=zh-CN" target="_self">
					<font size=4><i class="ai ai-google-scholar ai-3x"></i></font>
				</a>
			</div>
			<!-- <div style="display: inline-block; margin-right:0.0cm"">
					<a href=" https://scholar.google.com.hk/citations?user=sRcWOKUAAAAJ&hl=zh-CN" target="_self">
					<img src="assets/img/google-scholar-icon.png" alt="Google Scholar" title="Google Scholar" width="40px" />

					</a>
				</div> -->
			&emsp;
			<!-- Semantic Scholar -->
			<div style="display: inline-block; margin-top:0%">
				<a href="https://www.semanticscholar.org/author/Huanxuan-Liao/2232817310" target="_self">
					<font size=4><i class="ai ai-semantic-scholar ai-3x"></i></font>
				</a>
			</div>
			<!-- <div class="cropped" style="display: inline-block;">
					<a href=" https://www.semanticscholar.org/author/Huanxuan-Liao/2232817310" target="_self">
						<img src="assets/img/semantic-scholar-icon.png" style="position:relative;top: 20px;"
							alt="Semantic Scholar" title="Semantic Scholar" width="220px" />
					</a>
				</div> -->
			&emsp;
			<!-- DBLP -->
			<div style="display: inline-block; margin-top:0%">
				<a href="https://dblp.org/pid/355/1072.html" target="_self">
					<font size=4><i class="ai ai-dblp ai-3x"></i></font>
				</a>
			</div>
			&emsp;

			<!-- Openreview -->
			<div style="display: inline-block; margin-top:0%">
				<a href="https://openreview.net/profile?id=~Huanxuan_Liao1" target="_self">
					<font size=4><i class="ai ai-academia ai-3x"></i></font>
				</a>
			</div>
			&emsp;
			<!-- Mail -->
			<!-- <div style="display: inline-block; margin-bottom:0.0cm">
					<a href="mailto:huanxuanliao@gmail.com" target="_self">
						<font size=7><i class="fa fa-envelope" aria-hidden="true"></i>
						</font>
					</a>
				</div>
				&emsp; -->
			<!-- GitHub -->
			<div style="display: inline-block; margin-bottom:0.0cm">
				<a href="https://github.com/Xnhyacinth" target="_self">
					<font size=4><i class="fa fa-github fa-3x" class="dy" aria-hidden="true"></i>
					</font>
				</a>
			</div>
			<!-- <div style="display: inline-block; margin-right:0.0cm">
					<a href="https://github.com/Xnhyacinth" target="_self">
						<img src="https://upload.wikimedia.org/wikipedia/commons/c/c2/GitHub_Invertocat_Logo.svg"
							alt="Github" title="Github" width="48px" />
					</a>
				</div> -->
			&emsp;
			<!-- twitter -->
			<div style="display: inline-block; margin-top:0%">
				<a href="https://twitter.com/xn_hyacinth" target="_self">
					<font size=4><i aria-hidden="true" class="fa fa-twitter fa-3x"></i></font>
				</a>
			</div>
			&emsp;
			<!-- twitter -->
			<div style="display: inline-block; margin-top:0%">
				<a href="https://www.linkedin.com/in/huanxuan-liao-09ab8b341/" target="_self">
					<font size=4><i aria-hidden="true" class="fa fa-linkedin-square fa-3x"></i>
				</a>
			</div>
		</td>
		</td>
		</tr>
	</table>
	<br />
	<div style="margin-top: -0.6cm">
		<h2>
			<b>
				<font face="Roboto" size=+3>📝 Biography</font>
			</b>
		</h2>
		<!-- <hr /> -->
		<div class="divide"></div>
		<div class="app" style="margin-top: -0.0em">
			<p>
				<font size="4.5" face="Georgia">
					I'm a third-year PhD student at the Institute of Automation, Chinese Academy of Sciences
					(CASIA),
					supervised by Prof. <a href="https://heshizhu.github.io/">
						<font face="Georgia">Shizhu He</font>
					</a> and Prof. <a href="https://nlpr.ia.ac.cn/cip/~liukang/index.html">
						<font face="Georgia">Kang Liu</font>
					</a>.
					Before that, I have received B.Eng. from <a href="https://www.ncepu.edu.cn/">
						<font face="Georgia">North China
							Electric Power University (NCEPU)</font>
					</a> in 2023, under the supervision of
					Prof. Min Shi.
					<br><br>
					My research interests lie at <b>
						<font face="Georgia">Long Context Modeling</font>
					</b>, <b>
						<font face="Georgia">Efficient Reasoning</font>
					</b> and
					<b>
						<font face="Georgia">Efficient VLM</font>
					</b>.
					<br><br>
					Check my <a href="#publications">publications</a> and <a href="assets/CV.pdf">CV (PDF)</a> for
					more information!
				</font>
			</p>
		</div>
		<div class="app" style="margin-top: -1.2em">
			<div style="float:left;width:50%;">
				<h2><b>
						<font face="Roboto" size=+2>📚 Research Interests</font>
					</b></h2>
				<font size="4" face="Georgia" style="color: var(--color-ch);">
					<div style="margin-bottom:1%">
						&emsp;&emsp;<font size="4"><i class="fa fa-file-text-o"></i> Long Context Modeling</font>
					</div>
					&emsp;&emsp;&emsp;&emsp;<i class="fa fa-angle-right" style="color: #888;"></i>
					<font size="3.5" style="color: var(--color-low);"> Survey: <a
							href="https://github.com/Xnhyacinth/Long_Text_Modeling_Papers" target="_self"
							style="text-decoration: underline; color: inherit;">Awesome-LLM-LCLM</a>
						(Github Star > 1.7K)</font>
					<br>
					&emsp;&emsp;&emsp;&emsp;<i class="fa fa-angle-right" style="color: #888;"></i>
					<font size="3.5" style="color: var(--color-low);"> Long Context Compression: <a
							href="https://arxiv.org/abs/2505.15774" target="_self"
							style="text-decoration: underline; color: inherit;">HyCo2</a> (Arxiv 2025)</font>
					<br>
					&emsp;&emsp;&emsp;&emsp;<i class="fa fa-angle-right" style="color: #888;"></i>
					<font size="3.5" style="color: var(--color-low);"> KV Compression <a href="" target="_self"
							style="text-decoration: underline; color: inherit;">SparK</a> (Arxiv 2025)</font>
					<br>


					<div style="margin-bottom:1%; margin-top:1.3%">
						&emsp;&emsp;<font size="4"><i class="fa fa-file-text-o"></i> Efficient Reasoning</font>
					</div>
					&emsp;&emsp;&emsp;&emsp;<i class="fa fa-angle-right" style="color: #888;"></i>
					<font size="3.5" style="color: var(--color-low);"> Instruction Learning: <a
							href="https://arxiv.org/abs/2406.12382" target="_self"
							style="text-decoration: underline; color: inherit;"> TAGI</a> (NeurIPS 2024)
					</font>
					<br>
					&emsp;&emsp;&emsp;&emsp;<i class="fa fa-angle-right" style="color: #888;"></i>
					<font size="3.5" style="color: var(--color-low);"> Knowledge Internalization: <a
							href="https://arxiv.org/abs/2409.13183" target="_self"
							style="text-decoration: underline; color: inherit;"> SKIntern</a> (COLING
						2025)</font>
					<br>
					&emsp;&emsp;&emsp;&emsp;<i class="fa fa-angle-right" style="color: #888;"></i>
					<font size="3.5" style="color: var(--color-low);"> Knowledge Awakening: <a
							href="https://aclanthology.org/2025.coling-main.89/" target="_self"
							style="text-decoration: underline; color: inherit;"> AAG</a> (COLING
						2025)</font>
					<br>

					&emsp;&emsp;&emsp;&emsp;<i class="fa fa-angle-right" style="color: #888;"></i>
					<font size="3.5" style="color: var(--color-low);"> Knowledge Collaboration: <a
							href="https://arxiv.org/abs/2409.13203" target="_self"
							style="text-decoration: underline; color: inherit;"> NesyCD</a> (AAAI 2025)
					</font>
					<br>

					<!-- <div style="margin-bottom:1%; margin-top:1.3%">
						&emsp;&emsp;<font size="4"><i class="fa fa-file-text-o"></i> Continual Learning</font>
					</div> -->
					&emsp;&emsp;&emsp;&emsp;<i class="fa fa-angle-right" style="color: #888;"></i>
					<font size="3.5" style="color: var(--color-low);"> Knowledge Transfer (CL): <a
							href="https://arxiv.org/abs/2502.11482" target="_self"
							style="text-decoration: underline; color: inherit;"> DATA</a> (Arxiv
						2025)</font>
					<br>

					<div style="margin-top:1.3%">
						&emsp;&emsp;<font size="4"><i class="fa fa-file-text-o"></i> Efficient VLM</font>
					</div>
					<!-- <br> -->


					<!-- &emsp;&emsp;&emsp;&emsp;<i class="fa fa-angle-right" style="color: #888;"></i>
					<font size="3.5" style="color: var(--color-low);"> Dynamic Encoding  <a href="https://arxiv.org/abs/2409.13183"
							target="_self" style="text-decoration: underline; color: inherit;"></a></font>
					<br><br> -->
				</font>
				<!-- <p>
					<font size="4" face="Georgia">
						&emsp;&emsp;<i class="fa fa-file-text-o"></i>
						Large Language Model
						<br><br>
						
						&emsp;&emsp;<i class="fa fa-file-text-o"></i>
						Long Context Modeling
						<br>
						&emsp;&emsp;&emsp;&emsp;<i class="fa fa-angle-right" style="color: #888;"></i>
						<font size="3.5" style="color: var(--color-low);"> Survey: <a
								href="https://github.com/Xnhyacinth/Long_Text_Modeling_Papers" target="_self"
								style="text-decoration: underline; color: inherit;">Awesome-LLM-LCLM</a>
							(Github Star > 1.5K)</font>
						<br>
						&emsp;&emsp;&emsp;&emsp;<i class="fa fa-angle-right" style="color: #888;"></i>
						<font size="3.5" style="color: var(--color-low);"> Long Context Compression: <a
								href="https://arxiv.org/abs/2505.15774" target="_self"
								style="text-decoration: underline; color: inherit;">HyCo2</a></font>
						<br>
						&emsp;&emsp;&emsp;&emsp;<i class="fa fa-angle-right" style="color: #888;"></i>
						<font size="3.5" style="color: var(--color-low);"> KV Compression <a href="" target="_self"
								style="text-decoration: underline; color: inherit;"></a></font>
						<br><br>
						&emsp;&emsp;<i class="fa fa-file-text-o"></i>
						Efficient Reasoning
						<br>
						&emsp;&emsp;&emsp;&emsp;<i class="fa fa-angle-right" style="color: #888;"></i>
						<font size="3.5" style="color: var(--color-low);"> Internalizing CoT: <a href="https://arxiv.org/abs/2409.13183"
								target="_self" style="text-decoration: underline; color: inherit;">SKIntern</a></font>
						<br><br>
						&emsp;&emsp;<i class="fa fa-file-text-o"></i>
						Efficient VLM
						<br>
						&emsp;&emsp;&emsp;&emsp;<i class="fa fa-angle-right" style="color: #888;"></i>
						<font size="3.5" style="color: var(--color-low);"> Internalizing CoT: <a href="https://arxiv.org/abs/2409.13183"
								target="_self" style="text-decoration: underline; color: inherit;"></a></font>
						<br><br>
						<a href="https://github.com/Xnhyacinth/Long_Text_Modeling_Papers" target="_self">
							<font size="4" face="Georgia">Long Context
								Modeling</font>
						</a>
						<br><br>
						&emsp;&emsp;<i class="fa fa-file-text-o"></i>
						Context Compression
					</font>
				</p> -->
			</div>
			<div style="float:left;width:50%;">
				<h2>
					<font face="Roboto" size=+2>📖 Education & Internship</font>
				</h2>
				<p>
					<font size="4" face="Georgia">
						&emsp;&emsp;<i class="fa fa-briefcase"></i>
						Research Intern at <a href="https://www.bytedance.com" target="_self"><img
								src="assets/img/ByteDance_logo_English.svg" alt="bytedance Logo"
								style="border-radius: 10%; height: 22px; vertical-align: middle; margin: 0 5px;"
								border="0">
							<font size="4" face="Georgia">ByteDance</font>
						</a>, 2025.06-now
						<br>
						<font size="3">&emsp;&emsp;&emsp;&emsp;</font>
						<font size="4" style="color: var(--color-low);">Efficient VLM and Long Video Understanding
						</font>
						<br><br>
						&emsp;&emsp;<i class="fa fa-briefcase"></i>
						Research Intern at <a href="https://www.antgroup.com/" target="_self"><img
								src="assets/img/Ant_Group_logo.png" alt="Ant Group Logo"
								style="border-radius: 10%; height: 28px; vertical-align: middle; margin: 0 5px;"
								border="0">
							<font size="4" face="Georgia">Ant Group</font>
						</a>, 2025.02-2025.06
						<br>
						<font size="3">&emsp;&emsp;&emsp;&emsp;</font>
						<font size="4" style="color: var(--color-low);">Long Context Compression</font>
						<br><br>
						&emsp;&emsp;<i class="fa fa-graduation-cap"></i>
						PhD in Computer Science <a href="https://www.ucas.ac.cn" target="_self"><img
								src="assets/img/ucas.png" alt="ucas logo"
								style="border-radius: 10%; height: 32px; vertical-align: middle; margin: 0 5px 0 0; object-position: 0px 0; overflow: hidden; object-fit: cover;"
								border="0"></a>
						<a href="http://www.ia.cas.cn" target="_self"><img src="assets/img/casia.svg" alt="casia logo"
								style="border-radius: 10%; height: 32px; vertical-align: middle; margin: 0 5px 0 0; object-position: 0 0; overflow: hidden;"
								border="0"></a>, 2023-now
						<br>
						<font size="3">&emsp;&emsp;&emsp;&emsp;</font>
						<font size="4" style="color: var(--color-low);">Institute of Automation, Chinese Academy of
							Sciences</font>
						<br><br>
						&emsp;&emsp;<i class="fa fa-graduation-cap"></i>
						B.Eng. in Intelligence Science and Technology <a href="https://www.ncepu.edu.cn/"
							target="_self"><img src="assets/img/North_China_Electric_Power_University_logo.png"
								alt="ncepu logo"
								style="border-radius: 10%; height: 32px; vertical-align: middle; margin: 0 5px 0 0; object-position: 0 0; overflow: hidden;"
								border="0"></a>, 2019-2023
						<br>
						<font size="3">&emsp;&emsp;&emsp;&emsp;</font>
						<font size="4" style="color: var(--color-low);">North China Electric Power University, China
						</font>
					</font>
					<!-- <br><br> -->
					<!-- <br><br><br> -->
					<!-- <p>&emsp;&emsp;</p> -->
				</p>
			</div>

		</div>
		<br>
	</div>
	<!-- <h2><font face="Arial" size=+3>Research Interests</font></h2>
	<hr/>
	<p>
		<font size="4">
			&emsp;&emsp;<i class="fa fa-file-text-o"></i>
			Natural Language Processing
			<br><br>
			&emsp;&emsp;<i class="fa fa-file-text-o"></i>
			Reasoning
			<br><br>
			&emsp;&emsp;<i class="fa fa-file-text-o"></i>
			<a href="https://github.com/Xnhyacinth/Long_Text_Modeling_Papers" target="_self">Long Context Modeling & Length Extrapolation</a>
		</font>
	</p>
	<br> -->
	<br />
	<!-- 🔥 📒📝📖💻 📚 -->
	&nbsp;&nbsp;&nbsp;&nbsp;
	<br />
	<!-- 🔥 📒📝📖💻 📚 -->
	<!-- &nbsp;&nbsp;&nbsp;&nbsp; -->
	<div style="margin-top: -1em;">
		<h2>
			<b>
				<font face="Roboto" size=+3>🔥 Recent News
				</font>
			</b>
		</h2>
		<!-- <hr /> -->
		<div class="divide"></div>
		<ul>
			<li>
				<font size="4.5" face="Georgia">Dec, 2024. <a href="https://arxiv.org/abs/2409.13203"
						target="_self">Neural-Symbolic Collaborative Distillation: Advancing Small Language Models for
						Complex Reasoning Tasks</a> and <a href="https://arxiv.org/abs/2409.13202" target="_self">CITI:
						Enhancing Tool Utilizing Ability in Large Language Models without Sacrificing General
						Performance</a>
					accepted to <b class="news">AAAI 2025</b>!</font>
			</li>
			<br>
			<li>
				<font size="4.5" face="Georgia">Nov, 2024. <a href="https://aclanthology.org/2025.coling-main.89/"
						target="_self">Awakening Augmented Generation: Learning to Awaken Internal
						Knowledge of Large Language Models for Question Answering</a> and <a
						href="https://aclanthology.org/2025.coling-main.215/" target="_self">SKIntern: Internalizing
						Symbolic Knowledge for Distilling Better CoT Capabilities into Small Language Models</a>
					accepted to <b class="news">COLING 2025</b>!</font>
			</li>
			<br>
			<li>
				<font size="4.5" face="Georgia">Sep, 2024. <a href="https://openreview.net/forum?id=CluvZBfrjj"
						target="_self">From Instance Training to Instruction Learning: Task Adapters Generation from
						Instructions</a>
					accepted to <b class="news">NeurIPS 2024</b>!</font>
			</li>
			<br>
			<li>
				<font size="4.5" face="Georgia">Oct, 2023. <a
						href="https://link.springer.com/chapter/10.1007/978-981-99-7224-1_1" target="_self">Dynamic
						Weighted Neural Bellman-Ford Network for Knowledge Graph Reasoning </a>
					accepted to <b class="news">CCKS 2023</b>!</font>
			</li>
			<br>
			<li>
				<font size="4.5" face="Georgia">Jun, 2023. Graduated from NCEPU with honor: awarded <b
						class="news">Outstanding Graduate</b> by Beijing
					Ministry of
					Education and by NCEPU!</font>
			</li>
			<br>
			<li>
				<font size="4.5" face="Georgia">Dec, 2022. Awarded <b class="news">National Scholarship</b> by Ministry
					of Education!</font>
			</li>
			<br>
			<li>
				<font size="4.5" face="Georgia">Dec, 2021. Awarded <b class="news">National Scholarship</b> by Ministry
					of Education!</font>
			</li>
		</ul>
	</div>

	<br />

	<div style="margin-top: -1em;" id="publications">
		<h2>
			<b>
				<font face="Roboto" size=+3>📒 Publications</font>
			</b> &nbsp;<a href="https://scholar.google.com.hk/citations?user=sRcWOKUAAAAJ&hl=zh-CN" target="_self"><img
					src="../assets/img/google-scholar.png" alt="Google Scholar" width="8%" /></a>
		</h2>
		<!-- <hr /> -->
		<div class="divide"></div>
		<p>
			<font size="4" face="Georgia">Check my <a
					href="https://scholar.google.com.hk/citations?user=sRcWOKUAAAAJ&hl=zh-CN" target="_self">Google
					Scholar</a> for more information!
			</font>
		</p>
		<p>
			<font size="4" face="Georgia">(* stands for equal contribution. Listed in reverse chronological order.)
			</font>
		</p>
		<!-- <br /> -->

		<font size="5"><b>2025</b></font>

		<ul>
			<li>
				<b><a href="" class="paper">
						<font face=" Georgia" size="4">SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning</font>
					</a></b>
				<font size=" 4" face="Georgia">
					&nbsp;&nbsp;
					<br />
					<b class="myself">
						<font size=" 4" face=" Georgia">Huanxuan Liao</font>
					</b>
					, Yixing Xu, Shizhu He, Guanchen Li, Xuanwu Yin, Dong Li, Emad Barsoum, Jun Zhao, Kang Liu
					&nbsp;&nbsp; <br> <i><b>
							<font size=" 4" face="Georgia">Preprint</font>
						</b></i>, 2025.
				</font>
				<br>
				<div style="display: inline-block; margin-top:0.5%; width:50%">
					<div class="bu" id="15" style="display: inline-block; margin-top:0%">
						<font size="2"><i class="fa fa-info fa-2x" aria-hidden="true"></i></font>&nbsp;<img
							alt="Static Badge" src="https://img.shields.io/badge/Bib-5cb85c" style="height: 1em;">
					</div>
					&nbsp;
					<div class="au" id="15" style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<font size="5"><i class="fa fa-flag" aria-hidden="true"></i></font>&nbsp;<img alt="Static Badge"
							src="https://img.shields.io/badge/Abstract-f9adad" style="height: 1em;">
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="">
							<font size="5"><i class="ai ai-arxiv ai-1x"></i></font>&nbsp;<img alt="Static Badge"
								src="https://img.shields.io/badge/Arxiv-f0ad4e" style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a class="tag" href="cites/spark.bib" target="_self">
							<font size="2"><i class="fa fa-download fa-2x" aria-hidden="true"></i></font>&nbsp;<img
								id="2" alt="Static Badge" src="https://img.shields.io/badge/Cite-edd7be"
								style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://github.com/Xnhyacinth/SparK">
							<font size="2"><i class="fa fa-github-alt fa-2x" aria-hidden="true"></i></font>
							&nbsp;<img alt="Static Badge" src="https://img.shields.io/badge/Code/data-d9534f"
								style="height: 1em;">
						</a>
					</div>
					<!-- &nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://trae1oung.github.io/DyPRAG/">
							<font size="2"><i class="fa fa-home fa-2x" aria-hidden="true"></i></font> <img id="2"
								alt="Static Badge" src="https://img.shields.io/badge/Homepage-009fd0"
								style="height: 1em;">
						</a>
					</div> -->
				</div>

				<textarea id="mybib15" autoHeight="true" name="textarea" disabled class="bib15" style="display:none">
None</textarea>

				<textarea id="myab15" autoHeight="true" name="textarea" disabled class="ab15" style="display:none">
Long-context inference in large language models (LLMs) is increasingly constrained by the KV cache bottleneck: memory usage grows linearly with sequence length, while attention computation scales quadratically. Existing approaches address this issue by compressing the KV cache along the \textit{temporal} axis through strategies such as token eviction or merging to reduce memory and computational overhead. However, these methods often neglect fine-grained importance variations across feature dimensions (i.e., the \textit{channel axis}), thereby limiting their ability to effectively balance efficiency and model accuracy.In reality, we observe that channel saliency varies dramatically across both queries and positions: certain feature channels carry near-zero information for a given query, while others spike in relevance. To address this oversight, we propose \our, a training-free plug-and-play method that applies unstructured sparsity by pruning KV at the channel level, while dynamically restoring the pruned entries during attention score computation. Notably, our approach is orthogonal to existing KV compression and quantization techniques, making it compatible for integration with them to achieve further acceleration. By reducing channel-level redundancy, \our{} enables processing of longer sequences within the same memory budget. For sequences of equal length, \our{} not only preserves or improves model accuracy but also reduces KV cache storage by over $30\%$ compared to eviction-based methods. Furthermore, even in an aggressive pruning ratio of $80\%$, \our{} maintains performance with less degradation than $5\%$ compared to the based eviction method, demonstrating its robustness and effectiveness.</textarea>
			</li>
			<br />
			<li>
				<b><a href="https://arxiv.org/pdf/2505.15774" class="paper">
						<font face=" Georgia" size="4">Beyond Hard and Soft: Hybrid Context Compression for Balancing
							Local and Global Information Retention</font>
					</a></b>
				<font size=" 4" face="Georgia">
					&nbsp;&nbsp;
					<br />
					<b class="myself">
						<font size=" 4" face=" Georgia">Huanxuan Liao</font>
					</b>
					, Wen Hu, Yao Xu, Shizhu He, Jun Zhao, Kang Liu
					&nbsp;&nbsp; <br> <i><b>
							<font size=" 4" face="Georgia">Preprint</font>
						</b></i>, 2025.
				</font>
				<br>
				<div style="display: inline-block; margin-top:0.5%; width:50%">
					<div class="bu" id="14" style="display: inline-block; margin-top:0%">
						<font size="2"><i class="fa fa-info fa-2x" aria-hidden="true"></i></font>&nbsp;<img
							alt="Static Badge" src="https://img.shields.io/badge/Bib-5cb85c" style="height: 1em;">
					</div>
					&nbsp;
					<div class="au" id="14" style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<font size="5"><i class="fa fa-flag" aria-hidden="true"></i></font>&nbsp;<img alt="Static Badge"
							src="https://img.shields.io/badge/Abstract-f9adad" style="height: 1em;">
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://arxiv.org/abs/2505.15774">
							<font size="5"><i class="ai ai-arxiv ai-1x"></i></font>&nbsp;<img alt="Static Badge"
								src="https://img.shields.io/badge/Arxiv-f0ad4e" style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a class="tag" href="cites/hyco2.bib" target="_self">
							<font size="2"><i class="fa fa-download fa-2x" aria-hidden="true"></i></font>&nbsp;<img
								id="2" alt="Static Badge" src="https://img.shields.io/badge/Cite-edd7be"
								style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://github.com/Xnhyacinth/HyCo2">
							<font size="2"><i class="fa fa-github-alt fa-2x" aria-hidden="true"></i></font>
							&nbsp;<img alt="Static Badge" src="https://img.shields.io/badge/Code/data-d9534f"
								style="height: 1em;">
						</a>
					</div>
					<!-- &nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://trae1oung.github.io/DyPRAG/">
							<font size="2"><i class="fa fa-home fa-2x" aria-hidden="true"></i></font> <img id="2"
								alt="Static Badge" src="https://img.shields.io/badge/Homepage-009fd0"
								style="height: 1em;">
						</a>
					</div> -->
				</div>

				<textarea id="mybib14" autoHeight="true" name="textarea" disabled class="bib14" style="display:none">
@article{liao2025beyond,
  title={Beyond Hard and Soft: Hybrid Context Compression for Balancing Local and Global Information Retention},
  author={Liao, Huanxuan and Hu, Wen and Xu, Yao and He, Shizhu and Zhao, Jun and Liu, Kang},
  journal={arXiv preprint arXiv:2505.15774},
  year={2025}
}</textarea>

				<textarea id="myab14" autoHeight="true" name="textarea" disabled class="ab14" style="display:none">
Large Language Models (LLMs) encounter significant challenges in long-sequence inference due to computational inefficiency and redundant processing, driving interest in context compression techniques. Existing methods often rely on token importance to perform hard local compression or encode context into latent representations for soft global compression. However, the uneven distribution of textual content relevance and the diversity of demands for user instructions mean these approaches frequently lead to the loss of potentially valuable information. To address this, we propose Hybrid Context Compression (HyCo2) for LLMs, which integrates both global and local perspectives to guide context compression while retaining both the essential semantics and critical details for task completion. Specifically, we employ a hybrid adapter to refine global semantics with the global view, based on the observation that different adapters excel at different tasks. Then we incorporate a classification layer that assigns a retention probability to each context token based on the local view, determining whether it should be retained or discarded. To foster a balanced integration of global and local compression, we introduce auxiliary paraphrasing and completion pretraining before instruction tuning. This promotes a synergistic integration that emphasizes instruction-relevant information while preserving essential local details, ultimately balancing local and global information retention in context compression. Experiments show that our HyCo2 method significantly enhances long-text reasoning while reducing token usage. It improves the performance of various LLM series by an average of 13.1\% across seven knowledge-intensive QA benchmarks. Moreover, HyCo2 matches the performance of uncompressed methods while reducing token consumption by 88.8\%.</textarea>
			</li>
			<br />
			<li>
				<b><a href="https://arxiv.org/pdf/2502.11482" class="paper">
						<font face=" Georgia" size="4">DATA: Decomposed Attention-based Task Adaptation for
							Rehearsal-Free Continual Learning</font>
					</a></b>
				<font size=" 4" face="Georgia">
					&nbsp;&nbsp;
					<br />
					<b class="myself">
						<font size=" 4" face=" Georgia">Huanxuan Liao</font>
					</b>
					, Shizhu He, Yupu Hao, Jun Zhao, Kang Liu
					&nbsp;&nbsp; <br> <i><b>
							<font size=" 4" face="Georgia">Preprint</font>
						</b></i>, 2025.
				</font>
				<br>
				<div style="display: inline-block; margin-top:0.5%; width:50%">
					<div class="bu" id="9" style="display: inline-block; margin-top:0%">
						<font size="2"><i class="fa fa-info fa-2x" aria-hidden="true"></i></font>&nbsp;<img
							alt="Static Badge" src="https://img.shields.io/badge/Bib-5cb85c" style="height: 1em;">
					</div>
					&nbsp;
					<div class="au" id="9" style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<font size="5"><i class="fa fa-flag" aria-hidden="true"></i></font>&nbsp;<img alt="Static Badge"
							src="https://img.shields.io/badge/Abstract-f9adad" style="height: 1em;">
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://arxiv.org/abs/2502.11482">
							<font size="5"><i class="ai ai-arxiv ai-1x"></i></font>&nbsp;<img alt="Static Badge"
								src="https://img.shields.io/badge/Arxiv-f0ad4e" style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a class="tag" href="cites/DATA.bib" target="_self">
							<font size="2"><i class="fa fa-download fa-2x" aria-hidden="true"></i></font>&nbsp;<img
								id="2" alt="Static Badge" src="https://img.shields.io/badge/Cite-edd7be"
								style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://github.com/Xnhyacinth/DATA">
							<font size="2"><i class="fa fa-github-alt fa-2x" aria-hidden="true"></i></font>
							&nbsp;<img alt="Static Badge" src="https://img.shields.io/badge/Code/data-d9534f"
								style="height: 1em;">
						</a>
					</div>
					<!-- &nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://github.com/Xnhyacinth/DATA">
							<font size="2"><i class="fa fa-home fa-2x" aria-hidden="true"></i></font> <img id="2"
								alt="Static Badge" src="https://img.shields.io/badge/Homepage-009fd0"
								style="height: 1em;">
						</a>
					</div> -->
				</div>

				<textarea id="mybib9" autoHeight="true" name="textarea" disabled class="bib9" style="display:none">
@article{liao2025data,
	title={DATA: Decomposed Attention-based Task Adaptation for Rehearsal-Free Continual Learning},
	author={Liao, Huanxuan and He, Shizhu and Hao, Yupu and Zhao, Jun and Liu, Kang},
	journal={arXiv preprint arXiv:2502.11482},
	year={2025}
}</textarea>

				<textarea id="myab9" autoHeight="true" name="textarea" disabled class="ab9" style="display:none">
Continual learning (CL) is essential for Large Language Models (LLMs) to adapt to evolving real-world demands, yet they are susceptible to catastrophic forgetting (CF). While traditional CF solutions rely on expensive data rehearsal, recent rehearsal-free methods employ model-based and regularization-based strategies to address this issue. However, these approaches often neglect the model's plasticity, which is crucial to achieving optimal performance on newly learned tasks. Consequently, a key challenge in CL is striking a balance between preserving plasticity and mitigating CF. To tackle this challenge, we propose the Decomposed Attention-based Task Adaptation (DATA), which explicitly decouples and learns both task-specific and task-shared knowledge using high-rank and low-rank task adapters (e.g., LoRAs). For new tasks, DATA dynamically adjusts the weights of adapters of different ranks based on their relevance and distinction from previous tasks, allowing the model to acquire new task-specific skills while effectively retaining previously learned knowledge. Specifically, we implement a decomposed component weighting strategy comprising learnable components that collectively generate attention-based weights, allowing the model to integrate and utilize diverse knowledge from each DATA. Extensive experiments on three widely used benchmarks demonstrate that our proposed method achieves state-of-the-art performance. Notably, our approach significantly enhances model plasticity and mitigates CF by extending learnable components and employing stochastic restoration during training iterations.</textarea>
			</li>
			<br />
			<li>
				<b><a href="https://arxiv.org/pdf/2503.17407" class="paper">
						<font face=" Georgia" size="4">A Comprehensive Survey on Long Context Language Modeling</font>
					</a></b>
				<font size=" 4" face="Georgia">
					&nbsp;&nbsp;
					<br />
					LCLM-Horizon: Jiaheng Liu, Dawei Zhu, Zhiqi Bai, Yancheng He,
					<b class="myself">
						<font size=" 4" face=" Georgia">Huanxuan Liao</font>
					</b>
					, Haoran Que, Zekun Wang, Chenchen Zhang, Ge Zhang, Jiebin Zhang, Yuanxing Zhang, Zhuo Chen, Hangyu
					Guo, Shilong Li, Ziqiang Liu, Yong Shan, Yifan Song, Jiayi Tian, Wenhao Wu, Zhejian Zhou, Ruijie
					Zhu, Junlan Feng, Yang Gao, Shizhu He, Zhoujun Li, Tianyu Liu, Fanyu Meng, Wenbo Su, Yingshui Tan,
					Zili Wang, Jian Yang, Wei Ye, Bo Zheng, Wangchunshu Zhou, Wenhao Huang, Sujian Li, Zhaoxiang Zhang
					<!-- <b class="myself">
						<font size=" 4" face=" Georgia">Huanxuan Liao</font>
					</b>
					, Shizhu He, Yupu Hao, Jun Zhao, Kang Liu -->
					&nbsp;&nbsp; <br> <i><b>
							<font size=" 4" face="Georgia">Preprint</font>
						</b></i>, 2025.
				</font>
				<br>
				<div style="display: inline-block; margin-top:0.5%; width:50%">
					<div class="bu" id="12" style="display: inline-block; margin-top:0%">
						<font size="2"><i class="fa fa-info fa-2x" aria-hidden="true"></i></font>&nbsp;<img
							alt="Static Badge" src="https://img.shields.io/badge/Bib-5cb85c" style="height: 1em;">
					</div>
					&nbsp;
					<div class="au" id="12" style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<font size="5"><i class="fa fa-flag" aria-hidden="true"></i></font>&nbsp;<img alt="Static Badge"
							src="https://img.shields.io/badge/Abstract-f9adad" style="height: 1em;">
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://arxiv.org/abs/2503.17407">
							<font size="5"><i class="ai ai-arxiv ai-1x"></i></font>&nbsp;<img alt="Static Badge"
								src="https://img.shields.io/badge/Arxiv-f0ad4e" style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a class="tag" href="cites/lclm.bib" target="_self">
							<font size="2"><i class="fa fa-download fa-2x" aria-hidden="true"></i></font>&nbsp;<img
								id="2" alt="Static Badge" src="https://img.shields.io/badge/Cite-edd7be"
								style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a
							href="https://github.com/LCLM-Horizon/A-Comprehensive-Survey-For-Long-Context-Language-Modeling">
							<font size="2"><i class="fa fa-github-alt fa-2x" aria-hidden="true"></i></font>
							&nbsp;<img alt="Static Badge" src="https://img.shields.io/badge/Code/data-d9534f"
								style="height: 1em;">
						</a>
					</div>
					<!-- &nbsp;
				<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
					<a href="https://github.com/Xnhyacinth/DATA">
						<font size="2"><i class="fa fa-home fa-2x" aria-hidden="true"></i></font> <img id="2"
							alt="Static Badge" src="https://img.shields.io/badge/Homepage-009fd0"
							style="height: 1em;">
					</a>
				</div> -->
				</div>

				<textarea id="mybib12" autoHeight="true" name="textarea" disabled class="bib12" style="display:none">
@article{liu2025comprehensive,
	title={A Comprehensive Survey on Long Context Language Modeling},
	author={Liu, Jiaheng and Zhu, Dawei and Bai, Zhiqi and He, Yancheng and Liao, Huanxuan and Que, Haoran and Wang, Zekun and Zhang, Chenchen and Zhang, Ge and Zhang, Jiebin and others},
	journal={arXiv preprint arXiv:2503.17407},
	year={2025}
}</textarea>

				<textarea id="myab12" autoHeight="true" name="textarea" disabled class="ab12" style="display:none">
Efficient processing of long contexts has been a persistent pursuit in Natural Language Processing. With the growing number of long documents, dialogues, and other textual data, it is important to develop Long Context Language Models (LCLMs) that can process and analyze extensive inputs in an effective and efficient way. In this paper, we present a comprehensive survey on recent advances in long-context modeling for large language models. Our survey is structured around three key aspects: how to obtain effective and efficient LCLMs, how to train and deploy LCLMs efficiently, and how to evaluate and analyze LCLMs comprehensively. For the first aspect, we discuss data strategies, architectural designs, and workflow approaches oriented with long context processing. For the second aspect, we provide a detailed examination of the infrastructure required for LCLM training and inference. For the third aspect, we present evaluation paradigms for long-context comprehension and long-form generation, as well as behavioral analysis and mechanism interpretability of LCLMs. Beyond these three key aspects, we thoroughly explore the diverse application scenarios where existing LCLMs have been deployed and outline promising future development directions. This survey provides an up-to-date review of the literature on long-context LLMs, which we wish to serve as a valuable resource for both researchers and engineers.</textarea>
			</li>
			<br />
			<li>
				<b>[AAAI 2025]</b> <b><a href="https://arxiv.org/pdf/2409.13203" class="paper">
						<font face=" Georgia" size="4">Neural-Symbolic
							Collaborative Distillation: Advancing Small Language Models for Complex Reasoning Tasks
						</font>
					</a></b>
				&nbsp;&nbsp;
				<br />
				<font face="Georgia" font size=" 4">
					<b class="myself">
						<font size=" 4" face=" Georgia">Huanxuan Liao</font>
					</b>
					, Shizhu He, Yao Xu, Yuanzhe Zhang, Kang Liu, Jun Zhao
					&nbsp;&nbsp; <br>
					In <i><b>
							<font face="Georgia" font size=" 4">Proceedings of the 39th AAAI Conference on Artificial
								Intelligence</font>
						</b></i>.
				</font>
				<br>
				<div style="display: inline-block; margin-top:0.5%; width:50%">
					<div class="bu" id="6" style="display: inline-block; margin-top:0%">
						<font size="2"><i class="fa fa-info fa-2x" aria-hidden="true"></i></font>&nbsp;<img
							alt="Static Badge" src="https://img.shields.io/badge/Bib-5cb85c" style="height: 1em;">
					</div>
					&nbsp;
					<div class="au" id="6" style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<font size="5"><i class="fa fa-flag" aria-hidden="true"></i></font>&nbsp;<img alt="Static Badge"
							src="https://img.shields.io/badge/Abstract-f9adad" style="height: 1em;">
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://arxiv.org/abs/2409.13203">
							<font size="5"><i class="ai ai-arxiv ai-1x"></i></font>&nbsp;<img alt="Static Badge"
								src="https://img.shields.io/badge/Arxiv-f0ad4e" style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a class="tag" href="cites/NesyCD.bib" target="_self">
							<font size="2"><i class="fa fa-download fa-2x" aria-hidden="true"></i></font>&nbsp;<img
								id="2" alt="Static Badge" src="https://img.shields.io/badge/Cite-edd7be"
								style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://github.com/Xnhyacinth/NesyCD">
							<font size="2"><i class="fa fa-github-alt  fa-2x" aria-hidden="true"></i></font>&nbsp;<img
								alt="Static Badge" src="https://img.shields.io/badge/Code/data-d9534f"
								style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<!-- <div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://xnhyacinth.github.io/TAGI">
							<font size="2"><i class="fa fa-home fa-2x" aria-hidden="true"></i></font> <img id="2"
								alt="Static Badge" src="https://img.shields.io/badge/Homepage-009fd0"
								style="height: 1em;">
						</a>
					</div> -->
				</div>

				<textarea id="mybib6" autoHeight="true" name="textarea" disabled class="bib6" style="display:none">
@inproceedings{liao2025neural,
	title={Neural-symbolic collaborative distillation: Advancing small language models for complex reasoning tasks},
	author={Liao, Huanxuan and He, Shizhu and Xu, Yao and Zhang, Yuanzhe and Liu, Kang and Zhao, Jun},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={39},
	number={23},
	pages={24567--24575},
	year={2025}
}</textarea>

				<textarea id="myab6" autoHeight="true" name="textarea" disabled class="ab6" style="display:none">
In this paper, we propose Neural-Symbolic Collaborative Distillation (NesyCD), a novel knowledge distillation method for learning the complex reasoning abilities of Large Language Models (LLMs, e.g., \textgreater 13B). We argue that complex reasoning tasks are difficult for Small Language Models (SLMs, e.g., ≤ 7B), as these tasks demand not only general cognitive abilities but also specialized knowledge, which is often sparse and difficult for these neural-based SLMs to effectively capture. Therefore, NesyCD distills the general capabilities and specialized knowledge in LLMs using different manners. On the one hand, we distill only general abilities from teacher LLMs into the student SLMs of parameterized neural networks. On the other hand, for the specialized abilities and uncommon knowledge of a complex reasoning task, we employ a symbolic knowledge distillation approach to obtain and store the specialized knowledge within a symbolic knowledge base (KB). By decoupling general and specialized capabilities, the proposed NesyCD can achieve superior performance cost-effectively, utilizing smaller models and blending parameterized neural networks with symbolic KB. Moreover, the specialized KB generalizes well and is comprehended and manipulated by humans. Our experiments show that NesyCD significantly boosts SLMs' complex reasoning performance on in-domain (BBH, GSM8K) and out-of-domain (AGIEval, ARC) datasets. Notably, our approach enabled the LLaMA3-8B and Qwen2-7B to surpass GPT-3.5-turbo in performance and come close to matching LLaMA3-70B, despite the latter having nine times more parameters.</textarea>

			</li>
			<br />
			<li>
				<b>[COLING 2025]</b> <b><a href="https://aclanthology.org/2025.coling-main.215/" class="paper">
						<font face=" Georgia" size="4">SKIntern: Internalizing Symbolic Knowledge for Distilling
							Better CoT Capabilities into Small Language Models
						</font>
					</a></b>
				&nbsp;&nbsp;
				<br />
				<font face="Georgia" font size=" 4">
					<b class="myself">
						<font size=" 4" face=" Georgia">Huanxuan Liao</font>
					</b>
					, Shizhu He, Yupu Hao, Xiang Li, Yuanzhe Zhang, Jun Zhao, Kang Liu
					&nbsp;&nbsp; <br>
					In <i><b>
							<font face="Georgia" font size=" 4">Proceedings of the 31th International Conference on
								Computational Linguistics</font>
						</b></i>.
				</font>
				<br>
				<div style="display: inline-block; margin-top:0.5%; width:50%">
					<div class="bu" id="7" style="display: inline-block; margin-top:0%">
						<font size="2"><i class="fa fa-info fa-2x" aria-hidden="true"></i></font>&nbsp;<img
							alt="Static Badge" src="https://img.shields.io/badge/Bib-5cb85c" style="height: 1em;">
					</div>
					&nbsp;
					<div class="au" id="7" style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<font size="5"><i class="fa fa-flag" aria-hidden="true"></i></font>&nbsp;<img alt="Static Badge"
							src="https://img.shields.io/badge/Abstract-f9adad" style="height: 1em;">
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://arxiv.org/abs/2409.13183">
							<font size="5"><i class="ai ai-arxiv ai-1x"></i></font>&nbsp;<img alt="Static Badge"
								src="https://img.shields.io/badge/Arxiv-f0ad4e" style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a class="tag" href="cites/SKIntern.bib" target="_self">
							<font size="2"><i class="fa fa-download fa-2x" aria-hidden="true"></i></font>&nbsp;<img
								id="2" alt="Static Badge" src="https://img.shields.io/badge/Cite-edd7be"
								style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://github.com/Xnhyacinth/SKIntern">
							<font size="2"><i class="fa fa-github-alt  fa-2x" aria-hidden="true"></i></font>&nbsp;<img
								alt="Static Badge" src="https://img.shields.io/badge/Code/data-d9534f"
								style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<!-- <div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://xnhyacinth.github.io/TAGI">
							<font size="2"><i class="fa fa-home fa-2x" aria-hidden="true"></i></font> <img id="2"
								alt="Static Badge" src="https://img.shields.io/badge/Homepage-009fd0"
								style="height: 1em;">
						</a>
					</div> -->
				</div>

				<textarea id="mybib7" autoHeight="true" name="textarea" disabled class="bib7" style="display:none">
@inproceedings{liao2025skintern,
	title={SKIntern: Internalizing Symbolic Knowledge for Distilling Better CoT Capabilities into Small Language Models},
	author={Liao, Huanxuan and He, Shizhu and Hao, Yupu and Li, Xiang and Zhang, Yuanzhe and Zhao, Jun and Liu, Kang},
	booktitle={Proceedings of the 31st International Conference on Computational Linguistics},
	pages={3203--3221},	year={2025}
}</textarea>

				<textarea id="myab7" autoHeight="true" name="textarea" disabled class="ab7" style="display:none">
Small Language Models (SLMs) are attracting attention due to the high computational demands and privacy concerns of Large Language Models (LLMs). Some studies fine-tune SLMs using Chains of Thought (CoT) data distilled from LLMs, aiming to enhance their reasoning ability. Furthermore, Some CoT distillation methods introduce external symbolic knowledge into the generation process to improve the limited knowledge memory, reasoning ability and out-of-domain (OOD) generalization of SLMs. However, the introduction of symbolic knowledge increases computational overhead and introduces potential noise. In this paper, we introduce SKIntern, an innovative approach that empowers SLMs to internalize symbolic knowledge and few-shot examples gradually through a progressive fine-tuning process, guided by a predefined linear decay schedule under curriculum learning. By efficiently internalizing knowledge, SKIntern reduces computational overhead and speeds up the reasoning process by focusing solely on the question during inference. It outperforms state-of-the-art baselines by over 5\%, while reducing inference costs (measured in FLOPs) by up to 4× across a wide range of SLMs in both in-domain (ID) and out-of-domain (OOD) tasks.</textarea>

			</li>
			<br />
			<li>
				<b>[COLING 2025]</b> <b><a href="https://aclanthology.org/2025.coling-main.89/" class="paper">
						<font face=" Georgia" size="4">Awakening Augmented Generation: Learning to Awaken Internal
							Knowledge of Large Language Models for Question Answering</font>
					</a></b>
				&nbsp;&nbsp;
				<br />
				<font face="Georgia" font size=" 4">
					<b class="myself">
						<font size=" 4" face=" Georgia">Huanxuan Liao</font>
					</b>
					, Shizhu He, Yao Xu, Yuanzhe Zhang, Kang Liu, Shengping Liu, Jun
					Zhao
					&nbsp;&nbsp; <br>
					In <i><b>
							<font face="Georgia" font size=" 4">Proceedings of the 31th International Conference on
								Computational Linguistics</font>
						</b></i>.
				</font>
				<br>
				<div style="display: inline-block; margin-top:0.5%; width:50%">
					<div class="bu" id="3" style="display: inline-block; margin-top:0%">
						<font size="2"><i class="fa fa-info fa-2x" aria-hidden="true"></i></font>&nbsp;<img
							alt="Static Badge" src="https://img.shields.io/badge/Bib-5cb85c" style="height: 1em;">
					</div>
					&nbsp;
					<div class="au" id="3" style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<font size="5"><i class="fa fa-flag" aria-hidden="true"></i></font>&nbsp;<img alt="Static Badge"
							src="https://img.shields.io/badge/Abstract-f9adad" style="height: 1em;">
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://arxiv.org/abs/2403.15268">
							<font size="5"><i class="ai ai-arxiv ai-1x"></i></font>&nbsp;<img alt="Static Badge"
								src="https://img.shields.io/badge/Arxiv-f0ad4e" style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a class="tag" href="cites/IAG.bib" target="_self">
							<font size="2"><i class="fa fa-download fa-2x" aria-hidden="true"></i></font>&nbsp;<img
								id="2" alt="Static Badge" src="https://img.shields.io/badge/Cite-edd7be"
								style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://github.com/Xnhyacinth/IAG">
							<font size="2"><i class="fa fa-github-alt  fa-2x" aria-hidden="true"></i></font>
							&nbsp;<img alt="Static Badge" src="https://img.shields.io/badge/Code/data-d9534f"
								style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://xnhyacinth.github.io/IAG">
							<font size="2"><i class="fa fa-home fa-2x" aria-hidden="true"></i></font> <img id="2"
								alt="Static Badge" src="https://img.shields.io/badge/Homepage-009fd0"
								style="height: 1em;">
						</a>
					</div>
				</div>

				<textarea id="mybib3" autoHeight="true" name="textarea" disabled class="bib3" style="display:none">
@inproceedings{liao2025awakening,
	title={Awakening Augmented Generation: Learning to Awaken Internal Knowledge of Large Language Models for Question Answering},
	author={Liao, Huanxuan and He, Shizhu and Xu, Yao and Zhang, Yuanzhe and Liu, Shengping and Liu, Kang and Zhao, Jun},
	booktitle={Proceedings of the 31st International Conference on Computational Linguistics},
	pages={1333--1352},
	year={2025}
}</textarea>

				<textarea id="myab3" autoHeight="true" name="textarea" disabled class="ab3" style="display:none">
Retrieval-Augmented-Generation and Gener-ation-Augmented-Generation have been proposed to enhance the knowledge required for question answering over Large Language Models (LLMs). However, the former relies on external resources, and both require incorporating explicit documents into the context, which increases execution costs and susceptibility to noise data. Recent works indicate that LLMs have modeled rich knowledge, albeit not effectively triggered or awakened. Inspired by this, we propose a novel knowledge-augmented framework, Imagination-Augmented-Generation (IAG), which simulates the human capacity to compensate for knowledge deficits while answering questions solely through imagination, thereby awakening relevant knowledge in LLMs without relying on external resources. Guided by IAG, we propose an imagine richer context method for question answering (IMcQA). IMcQA consists of two modules: explicit imagination, which generates a short dummy document by learning from long context compression, and implicit imagination, which creates flexible adapters by distilling from a teacher model with a long context. Experimental results on three datasets demonstrate that IMcQA exhibits significant advantages in both open-domain and closed-book settings, as well as in out-of-distribution generalization.</textarea>
			</li>
			<br />
			<li>
				<b><a href="https://arxiv.org/pdf/2503.23895" class="paper">
						<font face=" Georgia" size="4">Better wit than wealth: Dynamic Parametric Retrieval Augmented
							Generation for Test-time Knowledge Enhancement</font>
					</a></b>
				<font size=" 4" face="Georgia">
					&nbsp;&nbsp;
					<br />
					Yuqiao Tan, Shizhu He,
					<b class="myself">
						<font size=" 4" face=" Georgia">Huanxuan Liao</font>
					</b>
					, Jun Zhao, Kang Liu
					<!-- <b class="myself">
						<font size=" 4" face=" Georgia">Huanxuan Liao</font>
					</b>
					, Shizhu He, Yupu Hao, Jun Zhao, Kang Liu -->
					&nbsp;&nbsp; <br> <i><b>
							<font size=" 4" face="Georgia">Preprint</font>
						</b></i>, 2025.
				</font>
				<br>
				<div style="display: inline-block; margin-top:0.5%; width:50%">
					<div class="bu" id="13" style="display: inline-block; margin-top:0%">
						<font size="2"><i class="fa fa-info fa-2x" aria-hidden="true"></i></font>&nbsp;<img
							alt="Static Badge" src="https://img.shields.io/badge/Bib-5cb85c" style="height: 1em;">
					</div>
					&nbsp;
					<div class="au" id="13" style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<font size="5"><i class="fa fa-flag" aria-hidden="true"></i></font>&nbsp;<img alt="Static Badge"
							src="https://img.shields.io/badge/Abstract-f9adad" style="height: 1em;">
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://arxiv.org/abs/2503.23895">
							<font size="5"><i class="ai ai-arxiv ai-1x"></i></font>&nbsp;<img alt="Static Badge"
								src="https://img.shields.io/badge/Arxiv-f0ad4e" style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a class="tag" href="cites/dyprag.bib" target="_self">
							<font size="2"><i class="fa fa-download fa-2x" aria-hidden="true"></i></font>&nbsp;<img
								id="2" alt="Static Badge" src="https://img.shields.io/badge/Cite-edd7be"
								style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://github.com/Trae1ounG/DyPRAG">
							<font size="2"><i class="fa fa-github-alt fa-2x" aria-hidden="true"></i></font>
							&nbsp;<img alt="Static Badge" src="https://img.shields.io/badge/Code/data-d9534f"
								style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://trae1oung.github.io/DyPRAG/">
							<font size="2"><i class="fa fa-home fa-2x" aria-hidden="true"></i></font> <img id="2"
								alt="Static Badge" src="https://img.shields.io/badge/Homepage-009fd0"
								style="height: 1em;">
						</a>
					</div>
				</div>

				<textarea id="mybib13" autoHeight="true" name="textarea" disabled class="bib13" style="display:none">
@article{tan2025better,
	title={Better wit than wealth: Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement},
	author={Tan, Yuqiao and He, Shizhu and Liao, Huanxuan and Zhao, Jun and Liu, Kang},
	journal={arXiv preprint arXiv:2503.23895},
	year={2025}
}</textarea>

				<textarea id="myab13" autoHeight="true" name="textarea" disabled class="ab13" style="display:none">
Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving relevant documents from external sources and incorporating them into the context. While it improves reliability by providing factual texts, it significantly increases inference costs as context length grows and introduces challenging issue of RAG hallucination, primarily caused by the lack of corresponding parametric knowledge in LLMs. An efficient solution is to enhance the knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by embedding document into LLMs parameters to perform test-time knowledge enhancement, effectively reducing inference costs through offline training. However, its high training and storage costs, along with limited generalization ability, significantly restrict its practical adoption. To address these challenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that leverages a lightweight parameter translator model to efficiently convert documents into parametric knowledge. DyPRAG not only reduces inference, training, and storage costs but also dynamically generates parametric knowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge conflicts in a plug-and-play manner at test-time. Extensive experiments on multiple datasets demonstrate the effectiveness and generalization capabilities of DyPRAG, offering a powerful and practical RAG paradigm which enables superior knowledge fusion and mitigates RAG hallucination in real-world applications.</textarea>
			</li>
			<br />
			<li>
				<b><a href="https://arxiv.org/pdf/2502.02315" class="paper">
						<font face=" Georgia" size="4">VaiBot: Shuttle Between the Instructions and Parameters of Large
							Language Models</font>
					</a></b>
				<font size=" 4" face="Georgia">
					&nbsp;&nbsp;
					<br />
					<font face="Georgia">Wangtao Sun, Haotian Xu, </font><b class="myself">
						<font face="Georgia">Huanxuan Liao</font>
					</b>
					, Xuanqing Yu, Zhongtao Jiang, Shizhu He, Jun Zhao, Kang Liu
					&nbsp;&nbsp; <br> <i><b>
							<font size=" 4" face="Georgia">Preprint</font>
						</b></i>, 2025.
				</font>
				<br>
				<div style="display: inline-block; margin-top:0.5%; width:50%">
					<div class="bu" id="8" style="display: inline-block; margin-top:0%">
						<font size="2"><i class="fa fa-info fa-2x" aria-hidden="true"></i></font>&nbsp;<img
							alt="Static Badge" src="https://img.shields.io/badge/Bib-5cb85c" style="height: 1em;">
					</div>
					&nbsp;
					<div class="au" id="8" style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<font size="5"><i class="fa fa-flag" aria-hidden="true"></i></font>&nbsp;<img alt="Static Badge"
							src="https://img.shields.io/badge/Abstract-f9adad" style="height: 1em;">
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://arxiv.org/abs/2502.02315">
							<font size="5"><i class="ai ai-arxiv ai-1x"></i></font>&nbsp;<img alt="Static Badge"
								src="https://img.shields.io/badge/Arxiv-f0ad4e" style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a class="tag" href="cites/VaiBot.bib" target="_self">
							<font size="2"><i class="fa fa-download fa-2x" aria-hidden="true"></i></font>&nbsp;<img
								id="2" alt="Static Badge" src="https://img.shields.io/badge/Cite-edd7be"
								style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://anonymous.4open.science/r/VaiBot-021F">
							<font size="2"><i class="fa fa-github-alt fa-2x" aria-hidden="true"></i></font>
							&nbsp;<img alt="Static Badge" src="https://img.shields.io/badge/Code/data-d9534f"
								style="height: 1em;">
						</a>
					</div>
					<!-- &nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://wengsyx.github.io/LMTuner/">
							<font size="2"><i class="fa fa-home fa-2x" aria-hidden="true"></i></font> <img id="2"
								alt="Static Badge" src="https://img.shields.io/badge/Homepage-009fd0"
								style="height: 1em;">
						</a>
					</div> -->
				</div>

				<textarea id="mybib8" autoHeight="true" name="textarea" disabled class="bib8" style="display:none">
@article{sun2025vaibot,
	title={VaiBot: Shuttle Between the Instructions and Parameters},
	author={Sun, Wangtao and Xu, Haotian and Liao, Huanxuan and Yu, Xuanqing and Jiang, Zhongtao and He, Shizhu and Zhao, Jun and Liu, Kang},
	journal={arXiv preprint arXiv:2502.02315},
	year={2025}
}</textarea>

				<textarea id="myab8" autoHeight="true" name="textarea" disabled class="ab8" style="display:none">
How to interact with LLMs through \emph{instructions} has been widely studied by researchers. However, previous studies have treated the emergence of instructions and the training of LLMs on task data as separate processes, overlooking the inherent unity between the two. This paper proposes a neural network framework, VaiBot, that integrates VAE and VIB, designed to uniformly model, learn, and infer both deduction and induction tasks under LLMs. Through experiments, we demonstrate that VaiBot performs on par with existing baseline methods in terms of deductive capabilities while significantly surpassing them in inductive capabilities. We also find that VaiBot can scale up using general instruction-following data and exhibits excellent one-shot induction abilities. We finally synergistically integrate the deductive and inductive processes of VaiBot. Through T-SNE dimensionality reduction, we observe that its inductive-deductive process significantly improves the distribution of training parameters, enabling it to outperform baseline methods in inductive reasoning tasks. </textarea>
			</li>
			<br />
			<li>
				<b><a href="" class="paper">
						<font face=" Georgia" size="4">Differentiated Instruction for Student Language Models:
							Contrastive Error Self-Correction via Refinement Feedback of Large Language Models</font>
					</a></b>
				<font size=" 4" face="Georgia">
					&nbsp;&nbsp;
					<br />
					<font face="Georgia">Xiangnnan Wu, Shizhu He, </font><b class="myself">
						<font face="Georgia">Huanxuan Liao</font>
					</b>
					, Jun Zhao, Kang Liu
					&nbsp;&nbsp; <br> <i><b>
							<font size=" 4" face="Georgia">Preprint</font>
						</b></i>, 2025.
				</font>
				<!-- <br> -->
				<div style="display: inline-block; margin-top:0.5%; width:50%">
					<!-- <div class="bu" id="10" style="display: inline-block; margin-top:0%">
						<font size="2"><i class="fa fa-info fa-2x" aria-hidden="true"></i></font>&nbsp;<img
							alt="Static Badge" src="https://img.shields.io/badge/Bib-5cb85c" style="height: 1em;">
					</div>
					&nbsp;
					<div class="au" id="10" style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<font size="5"><i class="fa fa-flag" aria-hidden="true"></i></font>&nbsp;<img alt="Static Badge"
							src="https://img.shields.io/badge/Abstract-f9adad" style="height: 1em;">
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://arxiv.org/abs/2502.02315">
							<font size="5"><i class="ai ai-arxiv ai-1x"></i></font>&nbsp;<img alt="Static Badge"
								src="https://img.shields.io/badge/Arxiv-f0ad4e" style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a class="tag" href="cites/VaiBot.bib" target="_self">
							<font size="2"><i class="fa fa-download fa-2x" aria-hidden="true"></i></font>&nbsp;<img
								id="2" alt="Static Badge" src="https://img.shields.io/badge/Cite-edd7be"
								style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://anonymous.4open.science/r/VaiBot-021F">
							<font size="2"><i class="fa fa-github-alt fa-2x" aria-hidden="true"></i></font>
							&nbsp;<img alt="Static Badge" src="https://img.shields.io/badge/Code/data-d9534f"
								style="height: 1em;">
						</a>
					</div> -->
					<!-- &nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://wengsyx.github.io/LMTuner/">
							<font size="2"><i class="fa fa-home fa-2x" aria-hidden="true"></i></font> <img id="2"
								alt="Static Badge" src="https://img.shields.io/badge/Homepage-009fd0"
								style="height: 1em;">
						</a>
					</div> -->
				</div>

				<textarea id="mybib11" autoHeight="true" name="textarea" disabled class="bib11" style="display:none">
@inproceedings{Sun2025VaiBotSB,
	title={VaiBot: Shuttle Between the Instructions and Parameters of Large Language Models},
	author={Wangtao Sun and Haotian Xu and Huanxuan Liao and Xuanqing Yu and Zhongtao Jiang and Shizhu He and Jun Zhao and Kang Liu},
	year={2025},
	url={https://api.semanticscholar.org/CorpusID:276106789}
}</textarea>

				<textarea id="myab11" autoHeight="true" name="textarea" disabled class="ab11" style="display:none">
How to interact with LLMs through \emph{instructions} has been widely studied by researchers. However, previous studies have treated the emergence of instructions and the training of LLMs on task data as separate processes, overlooking the inherent unity between the two. This paper proposes a neural network framework, VaiBot, that integrates VAE and VIB, designed to uniformly model, learn, and infer both deduction and induction tasks under LLMs. Through experiments, we demonstrate that VaiBot performs on par with existing baseline methods in terms of deductive capabilities while significantly surpassing them in inductive capabilities. We also find that VaiBot can scale up using general instruction-following data and exhibits excellent one-shot induction abilities. We finally synergistically integrate the deductive and inductive processes of VaiBot. Through T-SNE dimensionality reduction, we observe that its inductive-deductive process significantly improves the distribution of training parameters, enabling it to outperform baseline methods in inductive reasoning tasks. </textarea>
			</li>
			<br />
			<li>
				<b><a href="https://arxiv.org/pdf/2503.00771" class="paper">
						<font face=" Georgia" size="4">Evaluating Personalized Tool-Augmented LLMs from the Perspectives
							of Personalization and Proactivity</font>
					</a></b>
				<font size=" 4" face="Georgia">
					&nbsp;&nbsp;
					<br />
					<font face="Georgia">Yupu Hao, Pengfei Cao, Zhuoran Jin, </font><b class="myself">
						<font face="Georgia">Huanxuan Liao</font>
					</b>
					, Yubo Chen, Kang Liu, Jun Zhao
					&nbsp;&nbsp; <br> <i><b>
							<font size=" 4" face="Georgia">Preprint</font>
						</b></i>, 2025.
				</font>
				<br>
				<div style="display: inline-block; margin-top:0.5%; width:50%">
					<div class="bu" id="10" style="display: inline-block; margin-top:0%">
						<font size="2"><i class="fa fa-info fa-2x" aria-hidden="true"></i></font>&nbsp;<img
							alt="Static Badge" src="https://img.shields.io/badge/Bib-5cb85c" style="height: 1em;">
					</div>
					&nbsp;
					<div class="au" id="10" style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<font size="5"><i class="fa fa-flag" aria-hidden="true"></i></font>&nbsp;<img alt="Static Badge"
							src="https://img.shields.io/badge/Abstract-f9adad" style="height: 1em;">
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://arxiv.org/abs/2503.00771">
							<font size="5"><i class="ai ai-arxiv ai-1x"></i></font>&nbsp;<img alt="Static Badge"
								src="https://img.shields.io/badge/Arxiv-f0ad4e" style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a class="tag" href="cites/ETAPP.bib" target="_self">
							<font size="2"><i class="fa fa-download fa-2x" aria-hidden="true"></i></font>&nbsp;<img
								id="2" alt="Static Badge" src="https://img.shields.io/badge/Cite-edd7be"
								style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://github.com/hypasd-art/ETAPP">
							<font size="2"><i class="fa fa-github-alt fa-2x" aria-hidden="true"></i></font>
							&nbsp;<img alt="Static Badge" src="https://img.shields.io/badge/Code/data-d9534f"
								style="height: 1em;">
						</a>
					</div>
					<!-- &nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://wengsyx.github.io/LMTuner/">
							<font size="2"><i class="fa fa-home fa-2x" aria-hidden="true"></i></font> <img id="2"
								alt="Static Badge" src="https://img.shields.io/badge/Homepage-009fd0"
								style="height: 1em;">
						</a>
					</div> -->
				</div>

				<textarea id="mybib10" autoHeight="true" name="textarea" disabled class="bib10" style="display:none">
@article{hao2025evaluating,
	title={Evaluating Personalized Tool-Augmented LLMs from the Perspectives of Personalization and Proactivity},
	author={Hao, Yupu and Cao, Pengfei and Jin, Zhuoran and Liao, Huanxuan and Chen, Yubo and Liu, Kang and Zhao, Jun},
	journal={arXiv preprint arXiv:2503.00771},
	year={2025}
}</textarea>

				<textarea id="myab10" autoHeight="true" name="textarea" disabled class="ab10" style="display:none">
Personalized tool utilization is essential for aligning large language models (LLMs) with user preference in interaction scenarios with various tools. However, most of the current benchmarks primarily focus on either personalization of text generation or direct tool-utilizing, without considering both. In this work, we introduce a novel benchmark ETAPP for evaluating personalized tool invocation, establishing a sandbox environment, and a comprehensive dataset of 800 testing cases covering diverse user profiles. To improve the accuracy of our evaluation, we propose a key-point-based LLM evaluation method, mitigating biases in the LLM-as-a-judge system by manually annotating key points for each test case and providing them to LLM as the reference. Additionally, we evaluate the excellent LLMs and provide an in-depth analysis. Furthermore, we investigate the impact of different tool-invoking strategies on LLMs' personalization performance and the effects of fine-tuning in our task. The effectiveness of our preference-setting and key-point-based evaluation method is also validated. Our findings offer insights into improving personalized LLM agents.</textarea>
			</li>
			<br />
			<li>
				<b>[AAAI 2025]</b> <b><a href="https://arxiv.org/pdf/2409.13202" class="paper">
						<font face=" Georgia" size="4">CITI: Enhancing Tool
							Utilizing Ability in Large Language Models without Sacrificing General Performance</font>
					</a></b>
				&nbsp;&nbsp;
				<br />
				<font face="Georgia" font size=" 4">
					Yupu Hao, Pengfei Cao, Zhuoran Jin,
					<b class="myself">
						<font size=" 4" face=" Georgia">Huanxuan Liao</font>
					</b>
					, Yubo Chen, Kang Liu, Jun Zhao
					&nbsp;&nbsp; <br>
					In <i><b>
							<font face="Georgia" font size=" 4">Proceedings of the 39th AAAI Conference on Artificial
								Intelligence</font>
						</b></i>.
					<!-- &nbsp;&nbsp; <br>
					<i><b>
							<font face="Georgia" font size=" 4">Preprint</font>
						</b></i>, 2024. -->
				</font>
				<br>
				<div style="display: inline-block; margin-top:0.5%; width:50%">
					<div class="bu" id="5" style="display: inline-block; margin-top:0%">
						<font size="2"><i class="fa fa-info fa-2x" aria-hidden="true"></i></font>&nbsp;<img
							alt="Static Badge" src="https://img.shields.io/badge/Bib-5cb85c" style="height: 1em;">
					</div>
					&nbsp;
					<div class="au" id="5" style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<font size="5"><i class="fa fa-flag" aria-hidden="true"></i></font>&nbsp;<img alt="Static Badge"
							src="https://img.shields.io/badge/Abstract-f9adad" style="height: 1em;">
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://arxiv.org/abs/2409.13202">
							<font size="5"><i class="ai ai-arxiv ai-1x"></i></font>&nbsp;<img alt="Static Badge"
								src="https://img.shields.io/badge/Arxiv-f0ad4e" style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a class="tag" href="cites/CITI.bib" target="_self">
							<font size="2"><i class="fa fa-download fa-2x" aria-hidden="true"></i></font>&nbsp;<img
								id="2" alt="Static Badge" src="https://img.shields.io/badge/Cite-edd7be"
								style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://github.com/hypasd-art/CITI">
							<font size="2"><i class="fa fa-github-alt  fa-2x" aria-hidden="true"></i></font>&nbsp;<img
								alt="Static Badge" src="https://img.shields.io/badge/Code/data-d9534f"
								style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<!-- <div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://xnhyacinth.github.io/TAGI">
							<font size="2"><i class="fa fa-home fa-2x" aria-hidden="true"></i></font> <img id="2"
								alt="Static Badge" src="https://img.shields.io/badge/Homepage-009fd0"
								style="height: 1em;">
						</a>
					</div> -->
				</div>

				<textarea id="mybib5" autoHeight="true" name="textarea" disabled class="bib5" style="display:none">
@inproceedings{hao2025citi,
	title={CITI: Enhancing Tool Utilizing Ability in Large Language Models without Sacrificing General Performance},
	author={Hao, Yupu and Cao, Pengfei and Jin, Zhuoran and Liao, Huanxuan and Chen, Yubo and Liu, Kang and Zhao, Jun},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={39},
	number={22},
	pages={23996--24004},
	year={2025}
}</textarea>

				<textarea id="myab5" autoHeight="true" name="textarea" disabled class="ab5" style="display:none">
Tool learning enables the Large Language Models (LLMs) to interact with the external environment by invoking tools, enriching the accuracy and capability scope of LLMs. However, previous works predominantly focus on improving model's tool-utilizing accuracy and the ability to generalize to new, unseen tools, excessively forcing LLMs to adjust specific tool-invoking pattern without considering the harm to model's general performance. This deviates from the actual applications and original intention of integrating tools to enhance model. To tackle this problem, we dissect the capability trade-offs by examining the hidden representation changes and the gradient-based importance score of model's components. Based on the analysis result, we propose a Component Importance-based Tool-utilizing ability Injection method (CITI). According to the gradient-based importance score of different components, it alleviates the capability conflicts caused by fine-tuning process by applying distinct training strategies to different components. CITI applies Mixture-Of-LoRA (MOLoRA) for important components. Meanwhile, it fine-tunes the parameters of few components deemed less important in the backbone of the LLM, while keeping other parameters frozen. CITI can effectively enhance the model's tool-utilizing capability without excessively compromising its general performance. Experimental results demonstrate that our approach achieves outstanding performance across a range of evaluation metrics.</textarea>

			</li>
			<!-- <br /> -->

			<!-- <br /> -->
		</ul>

		<font size="5"><b>2024</b></font>

		<ul>
			<li>
				<b>[NeurIPS 2024]</b> <b><a href="https://openreview.net/forum?id=CluvZBfrjj" class="paper">
						<font face=" Georgia" size="4">From Instance Training to Instruction Learning: Task Adapters
							Generation from Instructions</font>
					</a></b>
				<!-- &nbsp;&nbsp; -->
				<br />
				<font face="Georgia" font size=" 4">
					<b class="myself">
						<font size=" 4" face=" Georgia">Huanxuan Liao</font>
					</b>
					, Shizhu He, Yao Xu, Yuanzhe Zhang, Yanchao Hao, Shengping Liu, Kang Liu, Jun Zhao
					&nbsp;&nbsp; <br>
					In <i><b>
							<font face="Georgia" font size=" 4">Advances in Neural Information Processing Systems 37
							</font>
						</b></i>.
				</font>
				<br>
				<div style="display: inline-block; margin-top:0.5%; width:50%">
					<div class="bu" id="4" style="display: inline-block; margin-top:0%">
						<font size="2"><i class="fa fa-info fa-2x" aria-hidden="true"></i></font>&nbsp;<img
							alt="Static Badge" src="https://img.shields.io/badge/Bib-5cb85c" style="height: 1em;">
					</div>
					&nbsp;
					<div class="au" id="4" style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<font size="5"><i class="fa fa-flag" aria-hidden="true"></i></font>&nbsp;<img alt="Static Badge"
							src="https://img.shields.io/badge/Abstract-f9adad" style="height: 1em;">
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://arxiv.org/abs/2406.12382">
							<font size="5"><i class="ai ai-arxiv ai-1x"></i></font>&nbsp;<img alt="Static Badge"
								src="https://img.shields.io/badge/Arxiv-f0ad4e" style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a class="tag" href="cites/TAGI.bib" target="_self">
							<font size="2"><i class="fa fa-download fa-2x" aria-hidden="true"></i></font>&nbsp;<img
								id="2" alt="Static Badge" src="https://img.shields.io/badge/Cite-edd7be"
								style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://github.com/Xnhyacinth/TAGI">
							<font size="2"><i class="fa fa-github-alt  fa-2x" aria-hidden="true"></i></font>
							&nbsp;<img alt="Static Badge" src="https://img.shields.io/badge/Code/data-d9534f"
								style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://xnhyacinth.github.io/TAGI">
							<font size="2"><i class="fa fa-home fa-2x" aria-hidden="true"></i></font> <img id="2"
								alt="Static Badge" src="https://img.shields.io/badge/Homepage-009fd0"
								style="height: 1em;">
						</a>
					</div>
				</div>

				<textarea id="mybib4" autoHeight="true" name="textarea" disabled class="bib4" style="display:none">
@article{liao2024instance,
	title={From instance training to instruction learning: Task adapters generation from instructions},
	author={Liao, Huanxuan and He, Shizhu and Xu, Yao and Zhang, Yuanzhe and Hao, Yanchao and Liu, Shengping and Liu, Kang and Zhao, Jun},
	journal={Advances in Neural Information Processing Systems},
	volume={37},
	pages={45552--45577},
	year={2024}
}</textarea>

				<textarea id="myab4" autoHeight="true" name="textarea" disabled class="ab4" style="display:none">
Large language models (LLMs) have acquired the ability to solve general tasks by utilizing instruction finetuning (IFT). However, IFT still relies heavily on instance training of extensive task data, which greatly limits the adaptability of LLMs to real-world scenarios where labeled task instances are scarce and broader task generalization becomes paramount. Contrary to LLMs, humans acquire skills and complete tasks not merely through repeated practice but also by understanding and following instructional guidelines. This paper is dedicated to simulating human learning to address the shortcomings of instance training, focusing on instruction learning to enhance cross-task generalization. Within this context, we introduce Task Adapters Generation from Instructions (TAGI), which automatically constructs the task-specific model in a parameter generation manner based on the given task instructions without retraining for unseen tasks. Specifically, we utilize knowledge distillation to enhance the consistency between TAGI developed through Learning with Instruction and task-specific models developed through Training with Instance, by aligning the labels, output logits, and adapter parameters between them. TAGI is endowed with cross-task generalization capabilities through a two-stage training process that includes hypernetwork pretraining and finetuning. We evaluate TAGI on the Super-Natural Instructions and P3 datasets. The experimental results demonstrate that TAGI can match or even outperform traditional meta-trained models and other hypernetwork models, while significantly reducing computational requirements.</textarea>

			</li>
			<!-- <br /> -->
		</ul>

		<font size="5"><b>2023</b></font>

		<ul>
			<li>
				<b><a href="https://arxiv.org/pdf/2308.10252.pdf" class="paper">
						<font face=" Georgia" size="4">LMTuner: An
							user-friendly and
							highly-integrable Training Framework for fine-tuning
							Large Language Models</font>
					</a></b>
				<font size=" 4" face="Georgia">
					&nbsp;&nbsp;
					<br />
					<font face="Georgia">Yixuan Weng, Zhiqi Wang, </font><b class="myself">
						<font face="Georgia">Huanxuan Liao</font>
					</b>
					, Shizhu He, Shengping Liu, Kang Liu, Jun Zhao
					&nbsp;&nbsp; <br> <i><b>
							<font size=" 4" face="Georgia">Preprint</font>
						</b></i>, 2023.
				</font>
				<br>
				<div style="display: inline-block; margin-top:0.5%; width:50%">
					<div class="bu" id="2" style="display: inline-block; margin-top:0%">
						<font size="2"><i class="fa fa-info fa-2x" aria-hidden="true"></i></font>&nbsp;<img
							alt="Static Badge" src="https://img.shields.io/badge/Bib-5cb85c" style="height: 1em;">
					</div>
					&nbsp;
					<div class="au" id="2" style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<font size="5"><i class="fa fa-flag" aria-hidden="true"></i></font>&nbsp;<img alt="Static Badge"
							src="https://img.shields.io/badge/Abstract-f9adad" style="height: 1em;">
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://arxiv.org/abs/2308.10252">
							<font size="5"><i class="ai ai-arxiv ai-1x"></i></font>&nbsp;<img alt="Static Badge"
								src="https://img.shields.io/badge/Arxiv-f0ad4e" style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a class="tag" href="cites/LMTuner.bib" target="_self">
							<font size="2"><i class="fa fa-download fa-2x" aria-hidden="true"></i></font>&nbsp;<img
								id="2" alt="Static Badge" src="https://img.shields.io/badge/Cite-edd7be"
								style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://github.com/WENGSYX/LMTuner">
							<font size="2"><i class="fa fa-github-alt fa-2x" aria-hidden="true"></i></font>
							&nbsp;<img alt="Static Badge" src="https://img.shields.io/badge/Code/data-d9534f"
								style="height: 1em;">
						</a>
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a href="https://wengsyx.github.io/LMTuner/">
							<font size="2"><i class="fa fa-home fa-2x" aria-hidden="true"></i></font> <img id="2"
								alt="Static Badge" src="https://img.shields.io/badge/Homepage-009fd0"
								style="height: 1em;">
						</a>
					</div>
				</div>

				<textarea id="mybib2" autoHeight="true" name="textarea" disabled class="bib2" style="display:none">
@article{weng2023lmtuner,
	title={LMTuner: An user-friendly and highly-integrable Training Framework for fine-tuning Large Language Models},
	author={Weng, Yixuan and Wang, Zhiqi and Liao, Huanxuan and He, Shizhu and Liu, Shengping and Liu, Kang and Zhao, Jun},
	journal={arXiv preprint arXiv:2308.10252},
	year={2023}
}</textarea>

				<textarea id="myab2" autoHeight="true" name="textarea" disabled class="ab2" style="display:none">
With the burgeoning development in the realm of large language models (LLMs), the demand for efficient incremental training tailored to specific industries and domains continues to increase. Currently, the predominantly employed frameworks lack modular design, it often takes a lot of coding work to kickstart the training of LLM. To address this, we present "LMTuner", a highly usable, integrable, and scalable system for training LLMs expeditiously and with minimal user-input. LMTuner comprises three main modules - the Interaction, Training, and Inference Modules. We advocate that LMTuner's usability and integrality alleviate the complexities in training large language models. Remarkably, even a novice user could commence training large language models within five minutes. Furthermore, it integrates DeepSpeed frameworks and supports Efficient Fine-Tuning methodologies like Low Rank Adaptation (LoRA), Quantized LoRA (QLoRA), etc., enabling the training of language models scaling from 300M to a whopping 130B parameters using a single server. The LMTuner's homepage (this https URL screencast video (this https URL) are now publicly available.</textarea>
				<!-- <div id="textarea" contenteditable="true" data-text="输入内容...">

				</div> -->
				<!-- <div class="ab2" style="display:none; width: 48em; border:1px solid #e8dada">
					<pre>
  With the burgeoning development in the realm of large language models (LLMs), the demand for efficient incremental training tailored to
  specific industries and domains continues to increase. Currently, the predominantly employed frameworks lack modular design, it often 
  takes a lot of coding work to kickstart the training of LLM. To address this, we present "LMTuner", a highly usable, integrable, and scal-
  able system for training LLMs expeditiously and with minimal user-input. LMTuner comprises three main modules - the Interaction, Train-
  ing, and Inference Modules. We advocate that LMTuner’s usability and integrality alleviate the complexities in training large language 
  models. Remarkably, even a novice user could commence raining large language models within five minutes. Furthermore, it integrates
  DeepSpeed frameworks and supports Efficient Fine-Tuning methodologies like Low Rank Adaptation (LoRA),Quantized LoRA (QLoRA), etc., 
  enabling the training of language models scaling from 300M to a whopping 130B parameters using a single server.
		</pre>
				</div> -->
			</li>
			<br />
			<li>
				<b>[CCKS 2023]</b> <b><a href="https://link.springer.com/chapter/10.1007/978-981-99-7224-1_1"
						class="paper">
						<font face="Georgia" size="4">Dynamic Weighted
							Neural Bellman-Ford Network for Knowledge Graph Reasoning</font>
					</a></b>
				<font size=" 4" face="Georgia">
					&nbsp;&nbsp; <br />
					<b class="myself">
						<font face="Georgia">Huanxuan Liao</font>
					</b>
					, Shizhu He, Yao Xu, Kang Liu, Jun Zhao
					&nbsp;&nbsp; <br> Accepted to
					<i> <b>
							<font face="Georgia">the 8th China Conference on Knowledge Graph and Semantic
								Computing</font>
						</b></i>.
				</font>
				<br>
				<div style="display: inline-block; margin-top:0.5%; width:50%">
					<div class="bu" id="1" style="display: inline-block; margin-top:0%">
						<font size="5"><i class="fa fa-info" aria-hidden="true"></i></font>&nbsp;<img alt="Static Badge"
							src="https://img.shields.io/badge/Bib-5cb85c" style="height:  1em;">
					</div>
					&nbsp;
					<div class="au" id="1" style="display: inline-block; margin-top:0%; margin-left: -1.1%;"
						onclick="autoResize('my-textarea1')">
						<font size="5"><i class="fa fa-flag" aria-hidden="true"></i></font>&nbsp;<img alt="Static Badge"
							src="https://img.shields.io/badge/Abstract-f9adad" style="height:  1em;">
					</div>
					&nbsp;
					<div style="display: inline-block; margin-top:0%; margin-left: -1.1%;">
						<a class="tag" href="cites/DyNBF.bib" target="_self">
							<font size="2"><i class="fa fa-download fa-2x" aria-hidden="true"></i></font>&nbsp;<img
								id="2" alt="Static Badge" src="https://img.shields.io/badge/Cite-edd7be"
								style="height: 1em;">
						</a>
					</div>
				</div>
				<!-- <a href=""><img alt="Static Badge" src="https://img.shields.io/badge/Arxiv-428BCA"
						style="width: 50px;"></a> -->
				<!-- <div class="bib1" style="display:none; width: 48em; border:1px solid #e8dada">
					<pre>
  @InProceedings{10.1007/978-981-99-7224-1_1,
	author="Liao, Huanxuan
	and He, Shizhu
	and Xu, Yao
	and Liu, Kang
	and Zhao, Jun",
	editor="Wang, Haofen
	and Han, Xianpei
	and Liu, Ming
	and Cheng, Gong
	and Liu, Yongbin
	and Zhang, Ningyu",
	title="Dynamic Weighted Neural Bellman-Ford Network for Knowledge Graph Reasoning",
	booktitle="Knowledge Graph and Semantic Computing: Knowledge Graph Empowers Artificial General Intelligence",
	year="2023",
	publisher="Springer Nature Singapore",
	address="Singapore",
	pages="3--16",
	isbn="978-981-99-7224-1"
  }												
					</pre>
				</div> -->

				<textarea id="mybib1" autoHeight="true" name="textarea" disabled class="bib1" style="display:none">
@inproceedings{liao2023dynamic,
	title={Dynamic Weighted Neural Bellman-Ford Network for Knowledge Graph Reasoning},
	author={Liao, Huanxuan and He, Shizhu and Xu, Yao and Liu, Kang and Zhao, Jun},
	booktitle={China Conference on Knowledge Graph and Semantic Computing},
	pages={3--16},
	year={2023},
	organization={Springer}
}</textarea>

				<textarea id="myab1" class="ab1" style="display:none;" autoHeight="true" disabled>
Recent studies have shown that subgraphs of the head entity, such as related relations and neighborhoods, are helpful for Knowledge Graph Reasoning (KGR). However, prior studies tend to focus solely on enhancing entity representations using related relations, with little attention paid to the impact of different relations on different entities and their importance in various reasoning paths. Meanwhile, conventional Graph Neural Networks (GNNs) utilized for KGR consider simultaneously neighboring nodes and connected relations of the head entity but typically use a standard message-passing paradigm over the entire Knowledge Graph (KG). This results in over-smoothed representations and limits efficiency. To address the above-mentioned limitations of existing methods, we propose a Dynamic Weighted Neural Bellman-Ford Network (DyNBF) for KGR, which utilizes relation weights generated from subgraphs to compute only the most relevant relations and entities. This way, we can integrate multiple reasoning paths more flexibly to achieve better interpretable reasoning, while scaling more easily to more complex and larger KGs. DyNBF consists of two key modules: 1) a transformer-based relation weights generator module, which computes the weights of different relations on the path with a sequence-to-sequence model, and 2) an NBFNet-based logic reasoner module, which obtains entity representations and conducts fact prediction with dynamic weights from the previous module. Empirical results on three standard KGR datasets demonstrate that the proposed approach can generate explainable reasoning paths and obtain competitive performance.</textarea>
				<!-- <div id="textarea_div" class="ab1" contenteditable="true" data-text="输入内容..." style="display:none;">
					Recent studies have shown that subgraphs of the head entity, such as related relations and neighborhoods, are helpful for Knowledge Graph Reasoning (KGR). However, prior studies tend to focus solely on enhancing entity representations using related relations, with little attention paid to the impact of different relations on different entities and their importance in various reasoning paths. Meanwhile, conventional Graph Neural Networks (GNNs) utilized for KGR consider simultaneously neighboring nodes and connected relations of the head entity but typically use a standard message-passing paradigm over the entire Knowledge Graph (KG). This results in over-smoothed representations and limits efficiency. To address the above-mentioned limitations of existing methods, we propose a Dynamic Weighted Neural Bellman-Ford Network (DyNBF) for KGR, which utilizes relation weights generated from subgraphs to compute only the most relevant relations and entities. This way, we can integrate multiple reasoning paths more flexibly to achieve better interpretable reasoning, while scaling more easily to more complex and larger KGs. DyNBF consists of two key modules: 1) a transformer-based relation weights generator module, which computes the weights of different relations on the path with a sequence-to-sequence model, and 2) an NBFNet-based logic reasoner module, which obtains entity representations and conducts fact prediction with dynamic weights from the previous module. Empirical results on three standard KGR datasets demonstrate that the proposed approach can generate explainable reasoning paths and obtain competitive performance.
				</div> -->
				<!-- <div class="ab1" style="display:none; width: 48em; border:1px solid #e8dada">
					<pre>
  Recent studies have shown that subgraphs of the head entity, such as related relations and neighborhoods, are helpful for Knowledge Graph
  Reasoning (KGR). However, prior studies tend to focus solelyon enhancing entity representations using related relations, with little attention 
  paid to the impact of different relations on different entities and their importance in various reasoning paths. Meanwhile, conventional Graph
  Neural Networks (GNNs) utilized for KGR consider simultaneously neighboring nodes and connected relations of the head entity but typically 
  use a standard message-passing paradigm over the entire Knowledge Graph (KG). This results in oversmoothed representations and limits 
  efficiency. To address the above-mentioned limitations of existing methods, we propose a Dynamic Weighted Neural Bellman-Ford Network 
  (DyNBF) for KGR, which utilizes relation weights generated from subgraphs to compute only the most relevant relations and entities. This 
  way, we can integrate multiple reasoning paths more flexibly to achieve better interpretable reasoning, while scaling more easily to more
  complex and larger KGs. DyNBF consists of two key modules: 1) a transformer-based relation weights generator module, which computes
  the weights of different relations on the path with a sequence-to-sequence model, and 2) an NBFNet-based logic reasoner module, which 
  obtains entity representations and conducts fact prediction with dynamic weights from the previous module. Empirical results on three stand-
  ard KGR datasets demonstratethat the proposed approach can generate explainable reasoning paths and obtain competitive performance.
			</pre>
				</div> -->
			</li>
		</ul>
	</div>
	<br />

	<div style="margin-top: -1em;" id="awards">
		<h2>
			<b>
				<font face="Roboto" size=+3>🎖️ Awards</font>
			</b>
		</h2>
		<!-- <hr /> -->
		<div class="divide"></div>
		<br />
		<font size="5"><b>Scholarship & Honors</b></font>

		<ul>
			<li>
				<font face=" Georgia" size="4"><b>Beijing Outstanding Graduate Awards</b> (<font face="宋体">北京市优秀毕业生
					</font>), Beijing Ministry of Education, 2023.
					<br>
					<i>Top graduate in Beijing.</i>
				</font>
			</li>
			<!-- <br> -->
			<li>
				<font face=" Georgia" size="4"><b>National Scholarship</b> (<font face="宋体">国家奖学金</font>), Ministry
					of Education, 2022.
					<br>
					<i>Top scholarship in China; 0.2% domestically.</i>
				</font>
			</li>
			<!-- <br> -->
			<li>
				<font face=" Georgia" size="4"><b>National Scholarship</b> (<font face="宋体">国家奖学金</font>), Ministry
					of Education, 2021.
					<br>
					<i>Top scholarship in China; 0.2% domestically.</i>
				</font>
			</li>
			<!-- <br> -->
			<li>
				<font face=" Georgia" size="4"><b>SiFang Society Scholarship</b> (<font face="宋体">四方社会奖学金</font>), North
					China Electric Power University, 2020.
					<br>
					<i>Top society scholarship in NCEPU.</i>
				</font>
			</li>
		</ul>
	</div>
	<!-- <br /> -->
	<font size="5"><b>Competition</b></font>

	<ul>
		<li>
			<font face=" Georgia" size="4"><b>National Third Prize</b> in Information Security Competition, 2022.
			</font>
		</li>
		<!-- <br> -->
		<li>
			<font face=" Georgia" size="4"><b>National Excellence</b> in College Student Innovation and Entrepreneurship
				Project, 2021.
			</font>
		</li>
		<!-- <br> -->
		<li>
			<font face=" Georgia" size="4"><b>Beijing Third Prize</b> in Internet +, 2021.
			</font>
		</li>
	</ul>
	</div>
	<br />
	<!-- 🔥 📒📝📖💻 📚 -->
	<!-- &nbsp;&nbsp;&nbsp;&nbsp; -->
	<!-- <h2><font face="Arial" size=+3>Website visit statistics</font></h2>
	<hr/>
	<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=500&t=tt&d=RDwpltapRj65Q7UM5LGnBHaIr57MgblmTCDCr6fHKo0'></script> -->
	<!-- 🔥 📒📝📖💻 📚 -->
	<!-- &nbsp;&nbsp;&nbsp;&nbsp; -->
	<div style="margin-top: -1em;" id="services">
		<h2>
			<b>
				<font face="Roboto" size=+3>💻 Services</font>
			</b>
		</h2>
		<!-- <hr /> -->
		<div class="divide"></div>
		<ul>
			<li><b>
					<font size="4.5" face="Georgia">Conference Reviewing
				</b>: ACL ARR 2024, 2025. NeurIPS 2025.</font>
			</li>
			<!-- <br /> -->
			<li><b>
					<font size="4.5" face="Georgia">Comunity
				</b>: <a
					href="https://github.com/Xnhyacinth/Awesome-LLM-Long-Context-Modeling">Awesome-LLM-Long-Context-Modeling
					Github</font>
				</a> 1.7k+ Stars.</font>
			</li>
		</ul>
	</div>

	<br />
	<div id="copyright" align="right">
		<br />
		<br />
		<p>Copyright&copy; 2024 Huanxuan Liao(<font face="宋体">廖桓萱</font>).</p>
	</div>
	<br />

</body>

</html>